{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data - pandas\n",
    "# Clean Data - clean_data: clean_data()\n",
    "# Process Data - adv_processing: convert_percentages(), convert_to_matchups()\n",
    "# Train & Evaluate model - train_model: split_data_to_test(), learn_from_data()\n",
    "# Make Predictions - train_model: get_nearest_neighbors(), get_norm_team_stats(), \n",
    "#                   get_matchup_sample(), sim_convert_to_matchups(), process_matchups_for_model(),\n",
    "#                   simulate_and_predict()\n",
    "# Summarize Predictions - ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/all_team_data.csv') # Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    }
   ],
   "source": [
    "# Clean Data \n",
    "from clean_data import clean_data \n",
    "from data_types import columns\n",
    "\n",
    "df = df[columns] # subset to selected columms\n",
    "df = clean_data(df) # adjust feature types, match school names, create game_ids, drop NAs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10809 entries, 0 to 11330\n",
      "Data columns (total 29 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   game         10809 non-null  int64  \n",
      " 1   game_result  10809 non-null  int64  \n",
      " 2   date         10809 non-null  object \n",
      " 3   school       10809 non-null  object \n",
      " 4   opp_team_id  10809 non-null  object \n",
      " 5   pts          10809 non-null  int64  \n",
      " 6   opp_pts      10809 non-null  int64  \n",
      " 7   ortg         10809 non-null  float64\n",
      " 8   drtg         10809 non-null  float64\n",
      " 9   pace         10809 non-null  float64\n",
      " 10  ftr          10809 non-null  float64\n",
      " 11  3par         10809 non-null  float64\n",
      " 12  ts_perc      10809 non-null  float64\n",
      " 13  trb_perc     10809 non-null  float64\n",
      " 14  ast_perc     10809 non-null  float64\n",
      " 15  stl_perc     10809 non-null  float64\n",
      " 16  blk_perc     10809 non-null  float64\n",
      " 17  efg_perc     10809 non-null  float64\n",
      " 18  tov_perc     10809 non-null  float64\n",
      " 19  orb_perc     10809 non-null  float64\n",
      " 20  ft_fga       10809 non-null  float64\n",
      " 21  srs          10809 non-null  float64\n",
      " 22  overtimes    10809 non-null  int64  \n",
      " 23  wins         10809 non-null  int64  \n",
      " 24  losses       10809 non-null  int64  \n",
      " 25  streak       10809 non-null  int64  \n",
      " 26  game_id      10809 non-null  object \n",
      " 27  away         10809 non-null  uint8  \n",
      " 28  home         10809 non-null  uint8  \n",
      "dtypes: float64(15), int64(8), object(4), uint8(2)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need a function for using means that gets the mean of the last k games\n",
    "k = 3\n",
    "list_of_games = []\n",
    "for game, team, id in zip(df['game'], df['school'], df['game_id']):\n",
    "    if game <= k:\n",
    "        continue\n",
    "    else: \n",
    "        current_stats = df[['game', 'date', 'game_result', 'school', 'opp_team_id', 'home', 'away', 'srs']][(df['school'] == team) & (df['game'] == game)]\n",
    "        current_stats['game_id'] = id\n",
    "        as_of_last_game = df[['streak', 'wins', 'losses']][(df['school'] == team) & (df['game'] == (game - 1))]\n",
    "        as_of_last_game['game_id'] = id\n",
    "        average_of_last_k = df[['pace', 'pts', 'opp_pts', 'ortg', 'drtg', 'ftr', '3par', 'ts_perc', 'trb_perc', 'ast_perc', 'stl_perc', \n",
    "            'blk_perc', 'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'overtimes']][(df['game'] >= (game - k)) & (df['game'] < game) & (df['school'] == team)].mean().to_frame().T\n",
    "        average_of_last_k['game_id'] = id\n",
    "        merged_row = pd.merge(pd.merge(current_stats, as_of_last_game, on='game_id'), average_of_last_k, on='game_id')\n",
    "        list_of_games.append(merged_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9525 entries, 0 to 0\n",
      "Data columns (total 29 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   game         9525 non-null   int64  \n",
      " 1   date         9525 non-null   object \n",
      " 2   game_result  9525 non-null   int64  \n",
      " 3   school       9525 non-null   object \n",
      " 4   opp_team_id  9525 non-null   object \n",
      " 5   home         9525 non-null   uint8  \n",
      " 6   away         9525 non-null   uint8  \n",
      " 7   srs          9525 non-null   float64\n",
      " 8   streak       9525 non-null   int64  \n",
      " 9   wins         9525 non-null   int64  \n",
      " 10  losses       9525 non-null   int64  \n",
      " 11  pace         9525 non-null   float64\n",
      " 12  pts          9525 non-null   float64\n",
      " 13  opp_pts      9525 non-null   float64\n",
      " 14  ortg         9525 non-null   float64\n",
      " 15  drtg         9525 non-null   float64\n",
      " 16  ftr          9525 non-null   float64\n",
      " 17  3par         9525 non-null   float64\n",
      " 18  ts_perc      9525 non-null   float64\n",
      " 19  trb_perc     9525 non-null   float64\n",
      " 20  ast_perc     9525 non-null   float64\n",
      " 21  stl_perc     9525 non-null   float64\n",
      " 22  blk_perc     9525 non-null   float64\n",
      " 23  efg_perc     9525 non-null   float64\n",
      " 24  tov_perc     9525 non-null   float64\n",
      " 25  orb_perc     9525 non-null   float64\n",
      " 26  ft_fga       9525 non-null   float64\n",
      " 27  overtimes    9525 non-null   float64\n",
      " 28  game_id      9525 non-null   object \n",
      "dtypes: float64(18), int64(5), object(4), uint8(2)\n",
      "memory usage: 2.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(list_of_games) # merge list into df\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data \n",
    "from adv_processing import only_duplicates, convert_percentages, convert_to_matchups\n",
    "\n",
    "df = only_duplicates(df) # keep only duplicate game_ids\n",
    "df = convert_percentages(df) # convert 0-100 percentages to 0-1\n",
    "df_match = convert_to_matchups(df) # convert df table to matchup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "\n",
    "from train_model import split_data_to_test\n",
    "\n",
    "# Split data based on date string for training and predictions\n",
    "df_match_train = split_data_to_test(df_match, date='2023-02-28', type='train')\n",
    "df_match_pred = split_data_to_test(df_match, date='2023-02-28', type='pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['game', 'date', 'game_result', 'school', 'opp_team_id', 'home', 'away',\n",
       "       'srs', 'streak', 'wins', 'losses', 'pace', 'pts', 'opp_pts', 'ortg',\n",
       "       'drtg', 'ftr', '3par', 'ts_perc', 'trb_perc', 'ast_perc', 'stl_perc',\n",
       "       'blk_perc', 'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'overtimes',\n",
       "       'game_id', 'opp_pts_self', 'opp_pts_team', 'opp_ortg', 'opp_drtg',\n",
       "       'opp_pace', 'opp_ftr', 'opp_3par', 'opp_ts_perc', 'opp_trb_perc',\n",
       "       'opp_ast_perc', 'opp_stl_perc', 'opp_blk_perc', 'opp_efg_perc',\n",
       "       'opp_tov_perc', 'opp_orb_perc', 'opp_ft_fga', 'opp_srs', 'opp_wins',\n",
       "       'opp_losses', 'opp_streak'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_match_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model \n",
    "# Subset training data \n",
    "df_match_train = df_match_train.drop(['game', 'date', 'school', 'opp_team_id', 'game_id'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardization Pipeline\n",
    "# Create a pipeline for numerical features\n",
    "num_transformer = Pipeline(steps=[('scaler', StandardScaler())]) \n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, ['pts', 'opp_pts', 'ortg', 'drtg','pace', 'ftr','3par', 'ts_perc', \n",
    "                                  'trb_perc', 'ast_perc', 'stl_perc', 'blk_perc',\n",
    "                                'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'opp_pts_self', 'opp_pts_team',\n",
    "                                'opp_ortg', 'opp_drtg', 'opp_pace','opp_ftr', 'opp_3par', 'opp_ts_perc', \n",
    "                                'opp_trb_perc', 'opp_ast_perc', 'opp_stl_perc', 'opp_blk_perc', \n",
    "                                'opp_efg_perc', 'opp_tov_perc', 'opp_orb_perc', 'opp_ft_fga'])\n",
    "        ], \n",
    "        remainder='passthrough'\n",
    "    )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.65      0.68       399\n",
      "           1       0.68      0.74      0.71       401\n",
      "\n",
      "    accuracy                           0.69       800\n",
      "   macro avg       0.70      0.69      0.69       800\n",
      "weighted avg       0.70      0.69      0.69       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df_match_train.drop('game_result', axis=1)\n",
    "y = df_match_train['game_result']\n",
    "y = y.astype('int')\n",
    "\n",
    "# split, train, test \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=235)\n",
    "\n",
    "\n",
    "# Scale Training Data\n",
    "X_train_scaled = preprocessor.fit_transform(X_train) # Scale Data\n",
    "    \n",
    "# Transform Test data \n",
    "X_test_scaled = preprocessor.transform(X_test) # Scale Data\n",
    "\n",
    "\n",
    "# Define the hyperparameters to use with GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [64],\n",
    "    'max_depth': [10],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [2],\n",
    "    'max_features': ['log2'],\n",
    "    'bootstrap': [True], \n",
    "    'oob_score': [True]\n",
    "    }\n",
    "\n",
    "# Define Random Forest Classifier \n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# Define Grid Search CV\n",
    "grid = GridSearchCV(rfc, param_grid, scoring='accuracy', cv=5, n_jobs=4)\n",
    "\n",
    "# Fit Grid Search\n",
    "grid.fit(X_train_scaled, y_train)  \n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "# Calibrate the classsifier using CalibratedClassifierCV\n",
    "calibrated_clf = CalibratedClassifierCV(grid, method='sigmoid', cv=5, n_jobs=4)\n",
    "calibrated_clf.fit(X_train_scaled, y_train)\n",
    "    \n",
    "# Predict probabilities for the test set using the calibrated classifier \n",
    "y_proba = calibrated_clf.predict_proba(X_test_scaled)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate performance of Random Forests Model \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = grid.predict(X_test_scaled)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37437603, 0.62562397])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(bootstrap=False, max_features=&#x27;log2&#x27;, min_samples_leaf=4,\n",
       "                       min_samples_split=5, n_estimators=128)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(bootstrap=False, max_features=&#x27;log2&#x27;, min_samples_leaf=4,\n",
       "                       min_samples_split=5, n_estimators=128)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, max_features='log2', min_samples_leaf=4,\n",
       "                       min_samples_split=5, n_estimators=128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction of rolling mean data\n",
    "df_match_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_outcomes = df_match_pred[['game_id', 'school', 'opp_team_id', 'game_result']] # dataframe of actual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset prediction data \n",
    "X_pred = df_match_pred.drop(['game', 'date', 'school', 'opp_team_id', 'game_id', 'game_result'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "X_pred_scaled = preprocessor.transform(X_pred) # Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2049486904.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_outcomes['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n"
     ]
    }
   ],
   "source": [
    "actual_outcomes['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2014807730.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_outcomes['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2014807730.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  actual_outcomes['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n"
     ]
    }
   ],
   "source": [
    "actual_outcomes['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "actual_outcomes['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6890459363957597"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_outcomes['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.78 \n",
      "Precision: 0.70\n",
      "FP Rate: 0.30\n"
     ]
    }
   ],
   "source": [
    "recall = actual_outcomes['tp'].sum() / actual_outcomes['game_result'].sum()\n",
    "precision = actual_outcomes['tp'].sum() / actual_outcomes['pred_pos'].sum()\n",
    "fp_rate = 1 - (actual_outcomes['tp'].sum() / actual_outcomes['pred_pos'].sum())\n",
    "\n",
    "print('Recall: {:.2f} \\nPrecision: {:.2f}\\nFP Rate: {:.2f}'.format(recall, precision, fp_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.69446934 0.69917649 0.70823899 0.69543239 0.70324686 0.70700275\n",
      " 0.70512775 0.70261792 0.69854756 0.70168632 0.70919418 0.70355739\n",
      " 0.70575079 0.69824292 0.70449096 0.70106722 0.70137186 0.70419418\n",
      " 0.70637775 0.70480936 0.70793829 0.69606132 0.70199882 0.70386006\n",
      " 0.69605149 0.69574882 0.70481722 0.70324292 0.70730936 0.70420204\n",
      " 0.70417649 0.70387186 0.70324292 0.70262186 0.69950472 0.70575472\n",
      " 0.6997956  0.70074489 0.70011399 0.70073703 0.69918436 0.70074292\n",
      " 0.69699489 0.70011989 0.70887579 0.70573703 0.70543042 0.70105739\n",
      " 0.6969772  0.70416667 0.70669025 0.69793239 0.71263561 0.70761989\n",
      " 0.70698899 0.70730739 0.71044615 0.70168042 0.70667256 0.70419222\n",
      " 0.70199489 0.70105346 0.70792846 0.70324686 0.70699292 0.70388168\n",
      " 0.70607115 0.71105346 0.70855936 0.71200472 0.70637579 0.70386989\n",
      " 0.70198899 0.70387775 0.69886006 0.70698703 0.70543436 0.70012382\n",
      " 0.70262775 0.70418042 0.69386596 0.70011006 0.69261596 0.69480936\n",
      " 0.70231329 0.70136596 0.70606132 0.70292846 0.69574096 0.69323703\n",
      " 0.70732115 0.70198703 0.70950472 0.70324489 0.69916667 0.70418042\n",
      " 0.70324292 0.70324686 0.70229167 0.70730346 0.69949882 0.70669222\n",
      " 0.70167846 0.69918239 0.70167453 0.69323506 0.70262775 0.70387972\n",
      " 0.70137186 0.70791863 0.70700275 0.70323506 0.70544025 0.70200275\n",
      " 0.70418632 0.70762775 0.69729756 0.70699882 0.70199292 0.70012775\n",
      " 0.69387382 0.70137579 0.70605936 0.69167649 0.70512579 0.69793829\n",
      " 0.70105936 0.70168632 0.70200275 0.69948703 0.70419025 0.70355936\n",
      " 0.70292453 0.70762972 0.70230739 0.70355936 0.69293239 0.69948899\n",
      " 0.70043436 0.70105346 0.70355346 0.70762382 0.70449686 0.70073506\n",
      " 0.70387775 0.70448899 0.7013522  0.70699686 0.70543632 0.70793829\n",
      " 0.70136989 0.7026081  0.70200079 0.70448506 0.70699292 0.69761596\n",
      " 0.70668042 0.70042256 0.70449686 0.70387382 0.69949292 0.70136203\n",
      " 0.70762775 0.70104756 0.70230346 0.70543436 0.70355739 0.70324882\n",
      " 0.70950472 0.70199686 0.70668239 0.70480346 0.70073703 0.70544418\n",
      " 0.70825079 0.70449686 0.7026081  0.70105346 0.71263561 0.71512579\n",
      " 0.70449882 0.70731722 0.70636792 0.70511792 0.69981132 0.70511792\n",
      " 0.70292649 0.70543239 0.70699096 0.70543239 0.70074686 0.70575079\n",
      " 0.69636792 0.70951061 0.70168632 0.70888168 0.70261596 0.70480149\n",
      " 0.70481132 0.70605542 0.70168042 0.70729953 0.70386989 0.70168239\n",
      " 0.70512382 0.70449686 0.70480936 0.70761989 0.70668632 0.69605149\n",
      " 0.70543042 0.70606525 0.70669025 0.70449292 0.70418632 0.70543436\n",
      " 0.70012775 0.69950275 0.70011792 0.70356132 0.70825275 0.70199096\n",
      " 0.70448899 0.70449686 0.70105346 0.70167846 0.70542649 0.70981329\n",
      " 0.69793239 0.69793042 0.70011792 0.70230739 0.70448703 0.6904206\n",
      " 0.70418436 0.70323506 0.70511792 0.69542649 0.70636596 0.70731722\n",
      " 0.70323506 0.70605346 0.69823899 0.70699686 0.70261596 0.70106329\n",
      " 0.70919222 0.70512775 0.70325472 0.70636989 0.70324686 0.70574882\n",
      " 0.70262775 0.71106525 0.70512382 0.70231132 0.70418239 0.70481132\n",
      " 0.69887775 0.69604953 0.69824489 0.70700865 0.70824489 0.70636596\n",
      " 0.70012775 0.70199882 0.69448703 0.70043829 0.70043239 0.70293239\n",
      " 0.70105346 0.70323899 0.70325079 0.70137186 0.69667453 0.70762186\n",
      " 0.70324292 0.70480936 0.69886596 0.70542846 0.69886006 0.70073703\n",
      " 0.70636399 0.70262186 0.70106329 0.70043042 0.70292846 0.70637775\n",
      " 0.70637382 0.70261989 0.70606329 0.70261399 0.69917846 0.70886792\n",
      " 0.70448506 0.70512579 0.70824292 0.69574096 0.6991706  0.70199489\n",
      " 0.70293632 0.69918239 0.70418632 0.70605739 0.71137579 0.70386596\n",
      " 0.70075472 0.70043632 0.70543239 0.70167649 0.71043436 0.70886989\n",
      " 0.70387972 0.70480739 0.71105542 0.70386006 0.70136596 0.70856525\n",
      " 0.70292256 0.70762186 0.70231132 0.70856132 0.70323703 0.69980542\n",
      " 0.70605936 0.70573506 0.70511399 0.69980149 0.70480936 0.70417649\n",
      " 0.70794615 0.70201651 0.70480739 0.70449882 0.70730739 0.70543436\n",
      " 0.70418829 0.70981132 0.70699489 0.70606525 0.70417846 0.70230542\n",
      " 0.70075079 0.7035456  0.70824489 0.70167256 0.70449489 0.70699686\n",
      " 0.70855936 0.70605346 0.70542256 0.70729953 0.70012382 0.69917846\n",
      " 0.70137186 0.70668239 0.71074686 0.70044222 0.70230936 0.71261989\n",
      " 0.70291667 0.70417846 0.70668239 0.70199882 0.69605346 0.70700472\n",
      " 0.69886596 0.70355346 0.70355936 0.70293042 0.69917846 0.70012775\n",
      " 0.70262382 0.70730542 0.70231132 0.70292846 0.70479363 0.70136399\n",
      " 0.70137775 0.69855542 0.70167453 0.70293239 0.70293632 0.70543436\n",
      " 0.70073899 0.70136203 0.69729363 0.69948703 0.70168239 0.70357115\n",
      " 0.7057331  0.70387382 0.70168239 0.70262382 0.70418042 0.70198899\n",
      " 0.69917453 0.70731525 0.70230149 0.70448703 0.69792846 0.7016706\n",
      " 0.70387579 0.70043436 0.70949686 0.6994831  0.70417846 0.70574489\n",
      " 0.70794418 0.70325275 0.70856329 0.70167453 0.70449096 0.70323899\n",
      " 0.70293042 0.70355739 0.69949292 0.70293239 0.70418632 0.71450275\n",
      " 0.70949292 0.69918239 0.70231132 0.69948703 0.69449686 0.70543239\n",
      " 0.70481722 0.70480542 0.70168436 0.70699686 0.7007331  0.69980149\n",
      "        nan 0.6969831         nan 0.70043632        nan 0.70512972\n",
      "        nan 0.70011203        nan 0.69980936        nan 0.69386596\n",
      "        nan 0.70042846        nan 0.69824292        nan 0.70261596\n",
      "        nan 0.69761399        nan 0.69417649        nan 0.70574686\n",
      "        nan 0.69448703        nan 0.70449686        nan 0.69948506\n",
      "        nan 0.70544025        nan 0.69948703        nan 0.69919025\n",
      "        nan 0.69479756        nan 0.70168239        nan 0.70668632\n",
      "        nan 0.70479756        nan 0.70449489        nan 0.69980346\n",
      "        nan 0.70543042        nan 0.69822917        nan 0.69950472\n",
      "        nan 0.70638168        nan 0.70542256        nan 0.70106525\n",
      "        nan 0.69855936        nan 0.70387775        nan 0.70826061\n",
      "        nan 0.70199096        nan 0.70607311        nan 0.7022956\n",
      "        nan 0.69574686        nan 0.70231525        nan 0.70199882\n",
      "        nan 0.69604953        nan 0.70198506        nan 0.69479363\n",
      "        nan 0.69949489        nan 0.70106132        nan 0.70106132\n",
      "        nan 0.70323506        nan 0.70418042        nan 0.70262186\n",
      "        nan 0.70294025        nan 0.69886399        nan 0.69730542\n",
      "        nan 0.70449882        nan 0.7010397         nan 0.70261203\n",
      "        nan 0.6960397         nan 0.70136989        nan 0.69824686\n",
      "        nan 0.69511006        nan 0.70104953        nan 0.70511792\n",
      "        nan 0.70730739        nan 0.69729756        nan 0.70042846\n",
      "        nan 0.70386989        nan 0.69981525        nan 0.69604363\n",
      "        nan 0.70168042        nan 0.70042453        nan 0.70198703\n",
      "        nan 0.70355739        nan 0.69918829        nan 0.70137186\n",
      "        nan 0.69919222        nan 0.69668042        nan 0.70512186\n",
      "        nan 0.70168829        nan 0.70325079        nan 0.70074292\n",
      "        nan 0.70199292        nan 0.70074489        nan 0.69823899\n",
      "        nan 0.70167846        nan 0.70199096        nan 0.69699096\n",
      "        nan 0.69573113        nan 0.70136203        nan 0.70543042\n",
      "        nan 0.69886203        nan 0.69949686        nan 0.69949686\n",
      "        nan 0.69511989        nan 0.70199686        nan 0.69229953\n",
      "        nan 0.70293042        nan 0.7035456         nan 0.69823899\n",
      "        nan 0.69635417        nan 0.70387186        nan 0.70262186\n",
      "        nan 0.70356132        nan 0.70074292        nan 0.69886006\n",
      "        nan 0.69729953        nan 0.70574096        nan 0.69730739\n",
      "        nan 0.7001081         nan 0.70294418        nan 0.69980739\n",
      "        nan 0.69511006        nan 0.69824489        nan 0.70605542\n",
      "        nan 0.70105149        nan 0.70262775        nan 0.70417846\n",
      "        nan 0.70512775        nan 0.70012382        nan 0.69636006\n",
      "        nan 0.69574292        nan 0.70011792        nan 0.70387579\n",
      "        nan 0.69979953        nan 0.70668042        nan 0.69604167\n",
      "        nan 0.70762775        nan 0.70324096        nan 0.69793239\n",
      "        nan 0.70230936        nan 0.70105346        nan 0.70512186\n",
      "        nan 0.69167256        nan 0.70012382        nan 0.70169222\n",
      "        nan 0.69854953        nan 0.69541863        nan 0.69792256\n",
      "        nan 0.70355542        nan 0.69918042        nan 0.70292846\n",
      "        nan 0.70293436        nan 0.70230542        nan 0.69636203\n",
      "        nan 0.6963581         nan 0.70199686        nan 0.70355346\n",
      "        nan 0.70292846        nan 0.7038581         nan 0.70419025\n",
      "        nan 0.70104953        nan 0.70324489        nan 0.70073703\n",
      "        nan 0.70198506        nan 0.6982331         nan 0.70699292\n",
      "        nan 0.70636989        nan 0.70075275        nan 0.70636596\n",
      "        nan 0.70824882        nan 0.70105149        nan 0.69949489\n",
      "        nan 0.70199096        nan 0.70668042        nan 0.6954206\n",
      "        nan 0.70043042        nan 0.70324096        nan 0.70386006\n",
      "        nan 0.70136203        nan 0.69823113        nan 0.70386989\n",
      "        nan 0.70387775        nan 0.70230542        nan 0.70293829\n",
      "        nan 0.70261792        nan 0.70292846        nan 0.70106329\n",
      "        nan 0.70105346        nan 0.70073899        nan 0.69761399\n",
      "        nan 0.70418042        nan 0.70106132        nan 0.69917453\n",
      "        nan 0.69824292        nan 0.70043436        nan 0.70230542\n",
      "        nan 0.70104953        nan 0.69510613        nan 0.70167256\n",
      "        nan 0.69979363        nan 0.70074096        nan 0.70293632\n",
      "        nan 0.70355542        nan 0.69667453        nan 0.70167649\n",
      "        nan 0.70011989        nan 0.70355739        nan 0.69823899\n",
      "        nan 0.70231132        nan 0.70544615        nan 0.70075275\n",
      "        nan 0.69668632        nan 0.70011203        nan 0.70230739\n",
      "        nan 0.70731722        nan 0.7007331         nan 0.69949292\n",
      "        nan 0.69481132        nan 0.70667649        nan 0.70230346\n",
      "        nan 0.6969831         nan 0.69980739        nan 0.70293239\n",
      "        nan 0.70575472        nan 0.70324096        nan 0.6994831\n",
      "        nan 0.70041863        nan 0.69667453        nan 0.69699686]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.67      0.69       380\n",
      "           1       0.71      0.74      0.73       420\n",
      "\n",
      "    accuracy                           0.71       800\n",
      "   macro avg       0.71      0.71      0.71       800\n",
      "weighted avg       0.71      0.71      0.71       800\n",
      "\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.69511399 0.69172182 0.70188454 0.70427009 0.70153061 0.69276062\n",
      " 0.70053319 0.70087332 0.70394834 0.7009009  0.69614589 0.69920482\n",
      " 0.69512318 0.69986441 0.69647454 0.69851305 0.69550239 0.69616428\n",
      " 0.7079955  0.69919562 0.70222697 0.70629482 0.7052882  0.70158577\n",
      " 0.7036036  0.69341791 0.70426319 0.70290265 0.70121346 0.70188913\n",
      " 0.70290724 0.69985291 0.69074738 0.70595238 0.70495725 0.70119048\n",
      " 0.70187994 0.69750873 0.70358062 0.70697509 0.69173791 0.70053089\n",
      " 0.70563063 0.70021373 0.7042494  0.71137847 0.70323129 0.70595238\n",
      " 0.70697049 0.6988348  0.701549   0.69783738 0.70259239 0.7042563\n",
      " 0.70088941 0.70763927 0.7025763  0.70020454 0.69784887 0.70391156\n",
      " 0.69918183 0.70222238 0.70527211 0.70190752 0.69004642 0.691754\n",
      " 0.698844   0.69850846 0.70428847 0.70019994 0.70797481 0.70629252\n",
      " 0.68629803 0.69410507 0.70157658 0.70292103 0.69681008 0.69271925\n",
      " 0.69817062 0.70459643 0.70327036 0.70088252 0.70087102 0.69513008\n",
      " 0.69750414 0.71104523 0.70156049 0.70629941 0.69476696 0.6981936\n",
      " 0.70291414 0.70055387 0.70293712 0.70222467 0.70428158 0.70562833\n",
      " 0.70293712 0.6961367  0.70121805 0.69986441 0.69917724 0.70156509\n",
      " 0.70291414 0.70223387 0.68938454 0.69950588 0.69985291 0.70123644\n",
      " 0.70290954 0.69917494 0.70119737 0.69816602 0.69851076 0.69511859\n",
      " 0.70256251 0.70359901 0.702574   0.69985981 0.7042563  0.705279\n",
      " 0.70022293 0.70019994 0.70427009 0.70223157 0.70087792 0.70967779\n",
      " 0.70731293 0.7015444  0.70088022 0.70122495 0.70223616 0.69546562\n",
      " 0.69952887 0.70560995 0.70086873 0.70392076 0.69749035 0.69919333\n",
      " 0.70155819 0.69985751 0.70630631 0.7005148  0.70527671 0.70493657\n",
      " 0.70087792 0.70190063 0.70461942 0.702574   0.70156968 0.70529049\n",
      " 0.7063086  0.70461482 0.70156968 0.70021144 0.70088252 0.70495036\n",
      " 0.7063086  0.70461482 0.70460103 0.70936064 0.70393685 0.7008963\n",
      " 0.70223387 0.70970537 0.70224306 0.70834253 0.7079909  0.70357603\n",
      " 0.70732212 0.69783508 0.7036105  0.7052813  0.70834023 0.7042517\n",
      " 0.69988279 0.70562603 0.70324508 0.69815453 0.70495955 0.70495266\n",
      " 0.69749035 0.7100501  0.70835402 0.70935374 0.70460563 0.70529969\n",
      " 0.70494806 0.69920712 0.7036059  0.69885549 0.7079932  0.71206564\n",
      " 0.70495036 0.70021373 0.70258549 0.70561454 0.71070279 0.70122035\n",
      " 0.70698658 0.70935144 0.6971663  0.71002942 0.70802307 0.70291414\n",
      " 0.7025763  0.70088022 0.70122955 0.70394604 0.70121576 0.70124563\n",
      " 0.70087332 0.70800469 0.70597077 0.70530888 0.70460792 0.7090205\n",
      " 0.69850616 0.69785347 0.70291184 0.69919103 0.71003631 0.70158577\n",
      " 0.70596847 0.70392995 0.70461022 0.7063155  0.70120656 0.69918643\n",
      " 0.70189373 0.70223846 0.70561454 0.70495036 0.70430916 0.70630171\n",
      " 0.7090228  0.69850156 0.70529279 0.70526981 0.70325657 0.71105902\n",
      " 0.69545413 0.70157198 0.69785576 0.69479684 0.70933995 0.70495036\n",
      " 0.70224306 0.7063132  0.69953576 0.70189143 0.69989888 0.69954265\n",
      " 0.70393914 0.70461022 0.70563982 0.70630171 0.69989658 0.697164\n",
      " 0.70122725 0.7063132  0.70666253 0.71206334 0.70459184 0.70699117\n",
      " 0.7073405  0.70088252 0.69886238 0.69646304 0.70427238 0.70188913\n",
      " 0.70427238 0.70494806 0.70021144 0.70089401 0.70495955 0.70326117\n",
      " 0.70495495 0.70088022 0.70935144 0.70598226 0.69887158 0.69920252\n",
      " 0.70022063 0.6988463  0.70256711 0.70359441 0.7052859  0.70358292\n",
      " 0.69480833 0.69952197 0.70224766 0.70428388 0.70089171 0.70595468\n",
      " 0.70731982 0.70358062 0.70020684 0.69581954 0.70054008 0.69681927\n",
      " 0.70593859 0.70900211 0.70866428 0.70426779 0.69919562 0.70394374\n",
      " 0.70461712 0.69848318 0.70562374 0.69852454 0.70426319 0.70493657\n",
      " 0.69986441 0.69614359 0.70392995 0.70666253 0.70088711 0.70019994\n",
      " 0.70460333 0.70697279 0.70020454 0.70529049 0.70224995 0.7025763\n",
      " 0.70934915 0.69952197 0.71137617 0.70290035 0.70019994 0.70393685\n",
      " 0.70325657 0.70391616 0.70054238 0.70021373 0.7042517  0.70189833\n",
      " 0.70390927 0.70224306 0.69954495 0.70089171 0.70834023 0.70529049\n",
      " 0.70427698 0.70663495 0.70259469 0.70188454 0.69785117 0.70864359\n",
      " 0.70393455 0.7015513  0.70864819 0.70291414 0.69950818 0.70289575\n",
      " 0.70088481 0.70324278 0.70832414 0.7117278  0.707984   0.70359671\n",
      " 0.7036105  0.69988049 0.69985981 0.69817981 0.69988509 0.70089171\n",
      " 0.70291644 0.69816372 0.70053548 0.70087102 0.69849007 0.70054468\n",
      " 0.70495036 0.69851765 0.70053778 0.70392306 0.69917264 0.69545872\n",
      " 0.69921401 0.70256941 0.70122955 0.70358752 0.70325198 0.7063109\n",
      " 0.69783508 0.69853374 0.69578507 0.70629711 0.70358522 0.70324738\n",
      " 0.70323129 0.70290954 0.70156509 0.70327036 0.70155819 0.70361969\n",
      " 0.69915655 0.70323129 0.70458954 0.70729454 0.69885319 0.70223846\n",
      " 0.70563982 0.70291414 0.699869   0.70157198 0.70255792 0.70460103\n",
      " 0.70766225 0.69985521 0.69849237 0.70832874 0.70359441 0.70292793\n",
      " 0.70222238 0.70288886 0.70359211 0.70835632 0.7025763  0.70868266\n",
      " 0.70561914 0.70800009 0.70868496 0.70189373 0.69917724 0.70119737\n",
      " 0.6988394  0.70698888 0.70020454 0.70426779 0.70325427 0.70324278\n",
      "        nan 0.68902831        nan 0.69816143        nan 0.69714791\n",
      "        nan 0.69816372        nan 0.68665196        nan 0.69444981\n",
      "        nan 0.69749724        nan 0.69444061        nan 0.70563523\n",
      "        nan 0.70018386        nan 0.69986211        nan 0.69985981\n",
      "        nan 0.69305709        nan 0.69545643        nan 0.70018616\n",
      "        nan 0.69918873        nan 0.69069682        nan 0.70123414\n",
      "        nan 0.69683306        nan 0.70630631        nan 0.69478764\n",
      "        nan 0.69815453        nan 0.70223157        nan 0.7052882\n",
      "        nan 0.69478305        nan 0.69579886        nan 0.70595698\n",
      "        nan 0.70053319        nan 0.69850386        nan 0.70256481\n",
      "        nan 0.70358981        nan 0.70593629        nan 0.70356453\n",
      "        nan 0.70287967        nan 0.69985062        nan 0.70392765\n",
      "        nan 0.69990347        nan 0.6981936         nan 0.69783508\n",
      "        nan 0.6988417         nan 0.69953576        nan 0.6998759\n",
      "        nan 0.70190063        nan 0.70085264        nan 0.69479454\n",
      "        nan 0.7015421         nan 0.69682157        nan 0.69953116\n",
      "        nan 0.69781899        nan 0.69545872        nan 0.699869\n",
      "        nan 0.70021833        nan 0.69139778        nan 0.69988509\n",
      "        nan 0.70018156        nan 0.69815913        nan 0.69717319\n",
      "        nan 0.69477386        nan 0.69749954        nan 0.70155589\n",
      "        nan 0.69885549        nan 0.69240899        nan 0.70085953\n",
      "        nan 0.70459643        nan 0.69647683        nan 0.69781899\n",
      "        nan 0.69817751        nan 0.70120656        nan 0.6971709\n",
      "        nan 0.70256941        nan 0.70120197        nan 0.70087562\n",
      "        nan 0.69615738        nan 0.70087332        nan 0.6998667\n",
      "        nan 0.70055847        nan 0.70155589        nan 0.69918873\n",
      "        nan 0.69884859        nan 0.70290954        nan 0.70324968\n",
      "        nan 0.70020224        nan 0.70054927        nan 0.69649292\n",
      "        nan 0.7036036         nan 0.70289345        nan 0.70088711\n",
      "        nan 0.69954495        nan 0.69817751        nan 0.69818671\n",
      "        nan 0.70428388        nan 0.7008963         nan 0.69681237\n",
      "        nan 0.69852225        nan 0.70225685        nan 0.69953346\n",
      "        nan 0.69308007        nan 0.70223846        nan 0.70325657\n",
      "        nan 0.69614129        nan 0.69581495        nan 0.6988348\n",
      "        nan 0.7009055         nan 0.69920482        nan 0.69814994\n",
      "        nan 0.70463091        nan 0.70359211        nan 0.70493657\n",
      "        nan 0.69580346        nan 0.70188454        nan 0.69512548\n",
      "        nan 0.7009009         nan 0.70459873        nan 0.69886008\n",
      "        nan 0.69917954        nan 0.70021144        nan 0.69683765\n",
      "        nan 0.7036036         nan 0.70224306        nan 0.69918873\n",
      "        nan 0.69683765        nan 0.70561914        nan 0.69918183\n",
      "        nan 0.70223387        nan 0.70189373        nan 0.70021603\n",
      "        nan 0.69817292        nan 0.70054008        nan 0.69309846\n",
      "        nan 0.69376494        nan 0.6998759         nan 0.70324738\n",
      "        nan 0.69581035        nan 0.69919562        nan 0.69850386\n",
      "        nan 0.70056076        nan 0.6934248         nan 0.69581495\n",
      "        nan 0.69852914        nan 0.6998759         nan 0.7025786\n",
      "        nan 0.69951967        nan 0.70088252        nan 0.70190982\n",
      "        nan 0.69883251        nan 0.69951737        nan 0.69444291\n",
      "        nan 0.70358292        nan 0.69919333        nan 0.70255792\n",
      "        nan 0.704254          nan 0.70054238        nan 0.69984832\n",
      "        nan 0.70153061        nan 0.70086183        nan 0.70390007\n",
      "        nan 0.70225915        nan 0.69475547        nan 0.69918873\n",
      "        nan 0.69849467        nan 0.70427698        nan 0.70054008\n",
      "        nan 0.70087332        nan 0.70223387        nan 0.69647913\n",
      "        nan 0.70018386        nan 0.70392995        nan 0.70190292\n",
      "        nan 0.69885549        nan 0.70596847        nan 0.70052629\n",
      "        nan 0.70153751        nan 0.70122955        nan 0.70256711\n",
      "        nan 0.70021603        nan 0.70153521        nan 0.70089171\n",
      "        nan 0.69477156        nan 0.70224995        nan 0.70763238\n",
      "        nan 0.70087792        nan 0.69649752        nan 0.69952657\n",
      "        nan 0.69511859        nan 0.70088022        nan 0.70122035\n",
      "        nan 0.69917494        nan 0.70662806        nan 0.69039345\n",
      "        nan 0.69714791        nan 0.70053319        nan 0.70358522\n",
      "        nan 0.70323359        nan 0.70427468        nan 0.70393685\n",
      "        nan 0.70324968        nan 0.70324049        nan 0.70393225\n",
      "        nan 0.69951737        nan 0.6998667         nan 0.70122955\n",
      "        nan 0.70324738        nan 0.69950359        nan 0.69918873\n",
      "        nan 0.69818441        nan 0.699869          nan 0.69850156\n",
      "        nan 0.70392535        nan 0.7063155         nan 0.69850846\n",
      "        nan 0.70153751        nan 0.70391156        nan 0.70427928\n",
      "        nan 0.69919103        nan 0.70121576        nan 0.70256251]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72       366\n",
      "           1       0.72      0.69      0.71       372\n",
      "\n",
      "    accuracy                           0.71       738\n",
      "   macro avg       0.71      0.71      0.71       738\n",
      "weighted avg       0.71      0.71      0.71       738\n",
      "\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=10,\n",
      "                       n_estimators=200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.70008856 0.69607128 0.70413536 0.69789878 0.70523562 0.69533061\n",
      " 0.7015672  0.71076374 0.70708459 0.69679047 0.70194826 0.69935863\n",
      " 0.69862334 0.70487065 0.71113407 0.7030351  0.7092851  0.69789609\n",
      " 0.70194289 0.71113944 0.70414073 0.71113139 0.71001771 0.70671157\n",
      " 0.70304047 0.69164878 0.70378113 0.69899367 0.70231054 0.700848\n",
      " 0.70671426 0.71002576 0.69789073 0.70082653 0.70047231 0.70707117\n",
      " 0.70671426 0.70010198 0.70414073 0.7137076  0.70598164 0.70451642\n",
      " 0.71186131 0.70854712 0.7092851  0.69826106 0.70708459 0.705614\n",
      " 0.7037704  0.71075301 0.70487334 0.71149635 0.70708727 0.70561936\n",
      " 0.70635198 0.70119418 0.70303778 0.70524098 0.70046694 0.6997397\n",
      " 0.70524635 0.69899635 0.71001503 0.70744687 0.70082385 0.70047499\n",
      " 0.70340812 0.70193484 0.7078172  0.70633856 0.71002576 0.71223164\n",
      " 0.69275708 0.70194826 0.69825837 0.69825569 0.7048787  0.70340275\n",
      " 0.70671962 0.70562205 0.70009929 0.70268087 0.70302973 0.70708459\n",
      " 0.70708995 0.70340275 0.69900708 0.7078172  0.70524635 0.7012076\n",
      " 0.71333727 0.70450032 0.70928778 0.71076106 0.70854981 0.70671157\n",
      " 0.699364   0.69752576 0.70047231 0.699364   0.69715812 0.70157256\n",
      " 0.70119687 0.7048787  0.70488407 0.70229712 0.70192411 0.69385734\n",
      " 0.70011271 0.70377576 0.70597359 0.69936131 0.69752845 0.69678778\n",
      " 0.70340275 0.70010734 0.70230249 0.70891745 0.71076374 0.70560863\n",
      " 0.70157256 0.70451106 0.7074415  0.70745492 0.70782256 0.70524098\n",
      " 0.70928242 0.70892014 0.69568216 0.70229981 0.70672499 0.7015672\n",
      " 0.70744687 0.70414609 0.70854981 0.71037999 0.69790146 0.70009392\n",
      " 0.69899098 0.69900172 0.70377845 0.70707922 0.71002845 0.70671426\n",
      " 0.70340812 0.70598433 0.70451374 0.70413804 0.70744687 0.70855517\n",
      " 0.71296157 0.70965812 0.71149635 0.70489212 0.71112334 0.70340812\n",
      " 0.70965812 0.71038267 0.70634929 0.70891745 0.7078172  0.70488139\n",
      " 0.70267282 0.71001771 0.71516477 0.70928778 0.70965275 0.70781451\n",
      " 0.70671157 0.70928778 0.70819289 0.70524367 0.71369955 0.70818753\n",
      " 0.71076106 0.71075569 0.70855249 0.70561131 0.71002576 0.7078172\n",
      " 0.71075837 0.70818216 0.70598433 0.70782256 0.70633856 0.70965006\n",
      " 0.70744687 0.70304315 0.70819289 0.70929584 0.70487602 0.70415414\n",
      " 0.70744955 0.70707653 0.70414341 0.70597896 0.70524903 0.70744955\n",
      " 0.71222091 0.70746028 0.70561668 0.70819021 0.70561131 0.70414609\n",
      " 0.71222896 0.70378113 0.70929315 0.70965543 0.70965006 0.70928778\n",
      " 0.71038536 0.70708459 0.70378113 0.70633856 0.70451374 0.70965006\n",
      " 0.70083459 0.70413804 0.71149367 0.69935863 0.70707922 0.70817679\n",
      " 0.71038804 0.70819021 0.70121297 0.70341885 0.70487065 0.70744955\n",
      " 0.70413804 0.70745223 0.70376771 0.70817948 0.70156451 0.7048787\n",
      " 0.71148293 0.7048787  0.69826642 0.70671426 0.70451106 0.70671157\n",
      " 0.70304315 0.70744955 0.7008319  0.70634929 0.70598164 0.70413804\n",
      " 0.71295889 0.70744687 0.70524635 0.70560595 0.70671962 0.70194289\n",
      " 0.70745223 0.71332922 0.70818216 0.70744418 0.70084264 0.70192948\n",
      " 0.70707385 0.70892014 0.70561936 0.70672231 0.7067062  0.70340006\n",
      " 0.70414341 0.70230786 0.70671426 0.70230786 0.70745492 0.70598164\n",
      " 0.70524635 0.70891745 0.69678778 0.70120223 0.70892014 0.70819021\n",
      " 0.70855786 0.71039341 0.70817948 0.70671426 0.71038804 0.70120223\n",
      " 0.70671426 0.7096608  0.70708459 0.70818484 0.70598701 0.70891745\n",
      " 0.71112602 0.70598164 0.7092851  0.70449764 0.70671694 0.70155646\n",
      " 0.71076642 0.70965812 0.69715812 0.70671426 0.71187205 0.69790146\n",
      " 0.70671962 0.71186131 0.71223701 0.71222359 0.6997397  0.70451374\n",
      " 0.70340275 0.7026755  0.70781183 0.70229981 0.70855249 0.71039073\n",
      " 0.7048787  0.70745223 0.70340275 0.70303778 0.70194289 0.705614\n",
      " 0.71039073 0.70928778 0.7008319  0.70450837 0.70892819 0.69606054\n",
      " 0.70965543 0.70193753 0.71333727 0.70891745 0.70229981 0.7034108\n",
      " 0.70560595 0.71002576 0.70782793 0.70819021 0.70965006 0.70928778\n",
      " 0.70230517 0.70340812 0.70193484 0.70156451 0.70928778 0.7136915\n",
      " 0.70781988 0.70635466 0.70120223 0.70082922 0.70229981 0.71185863\n",
      " 0.71075032 0.70929047 0.70634929 0.70488139 0.70671426 0.70782256\n",
      " 0.71370223 0.71186131 0.70928242 0.71149098 0.70707922 0.71075569\n",
      " 0.70378381 0.70377576 0.70010466 0.70635198 0.71002576 0.69789073\n",
      " 0.71590812 0.71075569 0.70010198 0.70267819 0.70560863 0.70965275\n",
      " 0.70634929 0.70451106 0.70671426 0.71332385 0.68981322 0.69642819\n",
      " 0.70561131 0.70524635 0.71149635 0.70487602 0.70928778 0.71074764\n",
      " 0.69863407 0.70451106 0.70230786 0.70378381 0.70670889 0.70414878\n",
      " 0.70230249 0.70413804 0.70378381 0.69937742 0.70781183 0.70156988\n",
      " 0.70230249 0.70450569 0.71002308 0.70524903 0.70819558 0.70120223\n",
      " 0.69641477 0.705614   0.70745492 0.70671157 0.70707653 0.70634392\n",
      " 0.70194021 0.7052383  0.70083727 0.70377308 0.7030351  0.70818216\n",
      " 0.70598164 0.70707385 0.70634929 0.69751771 0.71002576 0.70230786\n",
      " 0.71003113 0.70524903 0.70597628 0.70597359 0.70268087 0.6997397\n",
      " 0.7048787  0.71002576 0.71112334 0.70414609 0.71259661 0.7096608\n",
      "        nan 0.70230517        nan 0.69863407        nan 0.69863139\n",
      "        nan 0.70194558        nan 0.700848          nan 0.69715812\n",
      "        nan 0.70452179        nan 0.69863139        nan 0.70378113\n",
      "        nan 0.69899903        nan 0.70634661        nan 0.70560863\n",
      "        nan 0.70119687        nan 0.70156988        nan 0.70635198\n",
      "        nan 0.70598164        nan 0.69716885        nan 0.7034108\n",
      "        nan 0.70046962        nan 0.70560595        nan 0.70340275\n",
      "        nan 0.70451642        nan 0.70378381        nan 0.7089094\n",
      "        nan 0.70450032        nan 0.70230249        nan 0.70377845\n",
      "        nan 0.70671426        nan 0.70414878        nan 0.70929047\n",
      "        nan 0.70268087        nan 0.70339738        nan 0.70083459\n",
      "        nan 0.69972628        nan 0.70817948        nan 0.70744955\n",
      "        nan 0.69789609        nan 0.69753381        nan 0.7074576\n",
      "        nan 0.69825569        nan 0.70010734        nan 0.70045889\n",
      "        nan 0.69936936        nan 0.69827179        nan 0.69606054\n",
      "        nan 0.69569021        nan 0.6967851         nan 0.70083459\n",
      "        nan 0.70451374        nan 0.70229712        nan 0.70082922\n",
      "        nan 0.70193216        nan 0.69606054        nan 0.70634929\n",
      "        nan 0.70047499        nan 0.70083727        nan 0.69754991\n",
      "        nan 0.7048787         nan 0.70892014        nan 0.69788804\n",
      "        nan 0.70047767        nan 0.69825837        nan 0.70451106\n",
      "        nan 0.70193484        nan 0.70009929        nan 0.70119955\n",
      "        nan 0.70414609        nan 0.7100204         nan 0.69937473\n",
      "        nan 0.70816874        nan 0.70304584        nan 0.70818484\n",
      "        nan 0.69973433        nan 0.70598433        nan 0.70011003\n",
      "        nan 0.70377845        nan 0.69202179        nan 0.699364\n",
      "        nan 0.70635466        nan 0.70194021        nan 0.70304047\n",
      "        nan 0.70046157        nan 0.70524098        nan 0.70635734\n",
      "        nan 0.69826106        nan 0.70229981        nan 0.70230786\n",
      "        nan 0.70340275        nan 0.70046694        nan 0.69789341\n",
      "        nan 0.70046694        nan 0.70341617        nan 0.69974238\n",
      "        nan 0.70525172        nan 0.69826106        nan 0.70341348\n",
      "        nan 0.69973701        nan 0.70855517        nan 0.70268356\n",
      "        nan 0.70524903        nan 0.70304584        nan 0.70046694\n",
      "        nan 0.70121028        nan 0.70010466        nan 0.70819289\n",
      "        nan 0.70525708        nan 0.70745492        nan 0.70671962\n",
      "        nan 0.70046962        nan 0.70082653        nan 0.70157256\n",
      "        nan 0.70267014        nan 0.70413536        nan 0.70047767\n",
      "        nan 0.70304047        nan 0.70524635        nan 0.69605786\n",
      "        nan 0.70010734        nan 0.70854981        nan 0.70413536\n",
      "        nan 0.69900172        nan 0.70194021        nan 0.70010198\n",
      "        nan 0.7026755         nan 0.70194289        nan 0.69385198\n",
      "        nan 0.70524903        nan 0.69789341        nan 0.69532525\n",
      "        nan 0.69863139        nan 0.70450569        nan 0.69789341\n",
      "        nan 0.70635198        nan 0.70561668        nan 0.7078172\n",
      "        nan 0.70524098        nan 0.70084264        nan 0.7008319\n",
      "        nan 0.7026755         nan 0.70524635        nan 0.7030351\n",
      "        nan 0.70377845        nan 0.70598701        nan 0.70819021\n",
      "        nan 0.70046426        nan 0.70928778        nan 0.71149367\n",
      "        nan 0.7092851         nan 0.70046694        nan 0.70856054\n",
      "        nan 0.70487602        nan 0.70524635        nan 0.70451374\n",
      "        nan 0.70524903        nan 0.70597896        nan 0.70341348\n",
      "        nan 0.70451106        nan 0.70671426        nan 0.70488407\n",
      "        nan 0.70891745        nan 0.70560863        nan 0.70414878\n",
      "        nan 0.70377308        nan 0.70378113        nan 0.70560058\n",
      "        nan 0.70303778        nan 0.70671962        nan 0.71112602\n",
      "        nan 0.70451106        nan 0.7048787         nan 0.70157256\n",
      "        nan 0.70928778        nan 0.70524098        nan 0.70083459\n",
      "        nan 0.71001771        nan 0.71001771        nan 0.70412999\n",
      "        nan 0.71112065        nan 0.71002576        nan 0.70965275\n",
      "        nan 0.69825569        nan 0.7008319         nan 0.70634929\n",
      "        nan 0.70524098        nan 0.70304584        nan 0.70487334\n",
      "        nan 0.70486529        nan 0.70892014        nan 0.69421694\n",
      "        nan 0.70598433        nan 0.70155915        nan 0.70487334\n",
      "        nan 0.70707922        nan 0.70598433        nan 0.70780378\n",
      "        nan 0.70745223        nan 0.70377576        nan 0.70819021\n",
      "        nan 0.71406988        nan 0.70597628        nan 0.70965543\n",
      "        nan 0.70083459        nan 0.7100204         nan 0.70818484\n",
      "        nan 0.7052544         nan 0.70928242        nan 0.70744687\n",
      "        nan 0.70524635        nan 0.70856054        nan 0.70487602\n",
      "        nan 0.7008319         nan 0.70634929        nan 0.70523562\n",
      "        nan 0.70231054        nan 0.70155915        nan 0.70194021]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.69      0.69       341\n",
      "           1       0.69      0.69      0.69       340\n",
      "\n",
      "    accuracy                           0.69       681\n",
      "   macro avg       0.69      0.69      0.69       681\n",
      "weighted avg       0.69      0.69      0.69       681\n",
      "\n",
      "RandomForestClassifier(max_depth=10, max_features='log2', n_estimators=200,\n",
      "                       oob_score=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.69309524 0.69904762 0.69465079 0.70104762 0.69622857 0.70022857\n",
      " 0.70461587 0.70856508 0.70099683 0.70581905 0.70223175 0.69385079\n",
      " 0.70144127 0.70419048 0.70222222 0.7038127  0.6982254  0.69585397\n",
      " 0.70021905 0.70301587 0.69627619 0.70064762 0.7006127  0.69784762\n",
      " 0.70022857 0.7006381  0.70302857 0.705      0.70021587 0.70385714\n",
      " 0.70301905 0.7014381  0.69469206 0.68908571 0.69507619 0.69945079\n",
      " 0.70341587 0.7045873  0.69905714 0.69980317 0.69827619 0.69188571\n",
      " 0.70541905 0.7089746  0.70701905 0.69864444 0.69664762 0.7046254\n",
      " 0.69548889 0.69466032 0.69983492 0.70184127 0.70263492 0.70024127\n",
      " 0.7050127  0.7074     0.70421905 0.70182857 0.70143492 0.70144444\n",
      " 0.71456825 0.70340952 0.70301587 0.7054381  0.70579048 0.70421587\n",
      " 0.70144127 0.7058     0.70421587 0.70222222 0.70780952 0.69905079\n",
      " 0.69186032 0.68990476 0.69901587 0.69467619 0.69864127 0.69664762\n",
      " 0.70619365 0.70025397 0.69824444 0.69346667 0.70301587 0.70102222\n",
      " 0.70422222 0.69743492 0.69824444 0.69744127 0.70182857 0.69027937\n",
      " 0.69664762 0.70701905 0.70262857 0.70540635 0.70061587 0.70342857\n",
      " 0.68748571 0.68870476 0.70106349 0.7010254  0.69507302 0.6998381\n",
      " 0.70300317 0.69903175 0.68790159 0.69742222 0.69704444 0.70063492\n",
      " 0.70500635 0.70543492 0.69866349 0.69905397 0.70341905 0.70582857\n",
      " 0.70063492 0.6994127  0.7062     0.69983492 0.69506984 0.70383175\n",
      " 0.69823175 0.69903492 0.70582222 0.70460635 0.70463492 0.69985397\n",
      " 0.69866984 0.70659365 0.70779683 0.69026984 0.70183175 0.7042\n",
      " 0.71097143 0.69824762 0.7030127  0.70502857 0.69226349 0.70419048\n",
      " 0.70817778 0.70300317 0.70183175 0.7038127  0.70421905 0.7034254\n",
      " 0.70780635 0.70105079 0.70740952 0.70226667 0.70223492 0.7078127\n",
      " 0.70543492 0.70785079 0.70779365 0.70900952 0.70581905 0.70148254\n",
      " 0.70266032 0.70819683 0.71181905 0.70264444 0.69865079 0.70383492\n",
      " 0.70424127 0.70143175 0.70543492 0.7070381  0.70861587 0.70940635\n",
      " 0.69868571 0.70024762 0.711      0.7006127  0.7046254  0.70822222\n",
      " 0.7094127  0.71020952 0.70663492 0.70900635 0.70858413 0.70462222\n",
      " 0.7098     0.70424127 0.71180952 0.70503175 0.7094     0.69788571\n",
      " 0.70742857 0.7082127  0.7058381  0.70660317 0.71498095 0.7062127\n",
      " 0.70501905 0.70106667 0.70821905 0.70304444 0.71100952 0.7102\n",
      " 0.7006381  0.70185397 0.70741905 0.7054127  0.70581905 0.70742857\n",
      " 0.7026381  0.7138     0.70741905 0.70701905 0.7010381  0.70581587\n",
      " 0.70144127 0.70580317 0.7066254  0.7074127  0.71060317 0.7086254\n",
      " 0.70422222 0.69786349 0.70742857 0.70464127 0.70820635 0.7050254\n",
      " 0.70620635 0.70026032 0.69863492 0.70222222 0.70383492 0.7054254\n",
      " 0.70503492 0.70662222 0.70822857 0.70500317 0.70385714 0.71018095\n",
      " 0.70583175 0.70462222 0.70660952 0.71219683 0.71060635 0.7046381\n",
      " 0.70583492 0.7062127  0.70186349 0.7042254  0.70942222 0.70940635\n",
      " 0.71337778 0.7094254  0.70105079 0.70222222 0.70424127 0.70902857\n",
      " 0.7086     0.70580317 0.70346032 0.70621587 0.70182222 0.69626349\n",
      " 0.69946032 0.7062127  0.70981587 0.70303492 0.70780952 0.70185714\n",
      " 0.70898413 0.70063492 0.70703492 0.70702857 0.70941587 0.70184762\n",
      " 0.70501905 0.7062381  0.70064762 0.709      0.70263175 0.70942222\n",
      " 0.69866984 0.70500952 0.70543492 0.70740635 0.70543175 0.70224127\n",
      " 0.7141873  0.70977778 0.70065079 0.70501587 0.70660317 0.70424444\n",
      " 0.69783175 0.69506667 0.71379683 0.7006254  0.70381587 0.70505079\n",
      " 0.70062222 0.6978381  0.69582222 0.69502857 0.7006381  0.70142857\n",
      " 0.71059365 0.7042254  0.70739048 0.70899683 0.7022254  0.70062222\n",
      " 0.70541905 0.70259683 0.70743175 0.70302222 0.7042127  0.69984127\n",
      " 0.7002     0.70302222 0.70621905 0.70463175 0.69905079 0.7062127\n",
      " 0.69861905 0.70461587 0.69824762 0.70103175 0.69945079 0.70341905\n",
      " 0.7086254  0.69348254 0.70064444 0.71020635 0.70340952 0.69942857\n",
      " 0.69943492 0.70224127 0.70222222 0.70937143 0.70541905 0.7026127\n",
      " 0.7066381  0.70104127 0.70620635 0.7030381  0.707      0.70621905\n",
      " 0.70423492 0.70659683 0.70022857 0.70141905 0.7074     0.70777778\n",
      " 0.70341905 0.70697778 0.70819683 0.7066     0.69784762 0.70858095\n",
      " 0.7062254  0.69947302 0.70024444 0.70302222 0.70935556 0.70065397\n",
      " 0.69667302 0.69862857 0.70578413 0.69906032 0.6982127  0.70261587\n",
      " 0.6982381  0.69866032 0.69387619 0.69625714 0.7042127  0.70380317\n",
      " 0.69824127 0.70500635 0.7042127  0.70182857 0.69663175 0.7082\n",
      " 0.7058127  0.70900635 0.70066984 0.70184444 0.70101587 0.70065079\n",
      " 0.69628571 0.69386349 0.70023492 0.69384444 0.69902222 0.70064762\n",
      " 0.70818095 0.70301905 0.6994254  0.70783492 0.69664127 0.70222857\n",
      " 0.7038127  0.70022857 0.69784762 0.70658095 0.69662857 0.70462222\n",
      " 0.7074127  0.70141905 0.70383492 0.70303175 0.70820635 0.70579683\n",
      " 0.70223492 0.70223492 0.70026667 0.70145397 0.7042381  0.7025746\n",
      " 0.7018254  0.70061905 0.6986381  0.69866032 0.7042127  0.70382857\n",
      " 0.7030381  0.69664127 0.71138095 0.7030254  0.69466667 0.69785397\n",
      " 0.7026127  0.7002254  0.70583175 0.70781905 0.70422222 0.70262857\n",
      "        nan 0.68906667        nan 0.69744127        nan 0.69381905\n",
      "        nan 0.69502222        nan 0.69625397        nan 0.69346667\n",
      "        nan 0.69979365        nan 0.69145397        nan 0.69227619\n",
      "        nan 0.70459365        nan 0.69862222        nan 0.69861905\n",
      "        nan 0.70260952        nan 0.70460952        nan 0.69664444\n",
      "        nan 0.70103175        nan 0.70102222        nan 0.69504127\n",
      "        nan 0.70064127        nan 0.69544127        nan 0.69942857\n",
      "        nan 0.70302857        nan 0.69584444        nan 0.70180317\n",
      "        nan 0.69545714        nan 0.70144127        nan 0.69703492\n",
      "        nan 0.70023175        nan 0.69865397        nan 0.70343175\n",
      "        nan 0.7038127         nan 0.70022857        nan 0.69425397\n",
      "        nan 0.69427937        nan 0.70219683        nan 0.70061587\n",
      "        nan 0.68909206        nan 0.69465079        nan 0.69148254\n",
      "        nan 0.70339365        nan 0.69303492        nan 0.70100317\n",
      "        nan 0.69227619        nan 0.6990381         nan 0.69745079\n",
      "        nan 0.70462222        nan 0.69901905        nan 0.70022857\n",
      "        nan 0.69662222        nan 0.69384444        nan 0.69862857\n",
      "        nan 0.69624444        nan 0.69026032        nan 0.69584762\n",
      "        nan 0.6930381         nan 0.69982857        nan 0.68908571\n",
      "        nan 0.69786032        nan 0.70260952        nan 0.70221587\n",
      "        nan 0.68707937        nan 0.70262857        nan 0.70341587\n",
      "        nan 0.70339365        nan 0.69785079        nan 0.69825079\n",
      "        nan 0.70061587        nan 0.70816825        nan 0.69384762\n",
      "        nan 0.70898095        nan 0.69784762        nan 0.7038\n",
      "        nan 0.69665079        nan 0.70264762        nan 0.70581587\n",
      "        nan 0.70781587        nan 0.69786032        nan 0.7030381\n",
      "        nan 0.70461905        nan 0.7062254         nan 0.7058254\n",
      "        nan 0.70461905        nan 0.70341587        nan 0.70542222\n",
      "        nan 0.70223175        nan 0.70462222        nan 0.6998254\n",
      "        nan 0.70223492        nan 0.70502857        nan 0.70820635\n",
      "        nan 0.70263492        nan 0.70343492        nan 0.70143492\n",
      "        nan 0.7026381         nan 0.7078            nan 0.70542857\n",
      "        nan 0.70979048        nan 0.70384762        nan 0.70145714\n",
      "        nan 0.71099683        nan 0.70304444        nan 0.70781587\n",
      "        nan 0.70264444        nan 0.70182857        nan 0.7073873\n",
      "        nan 0.70462222        nan 0.70343175        nan 0.70620317\n",
      "        nan 0.69984127        nan 0.7050254         nan 0.70542222\n",
      "        nan 0.70900317        nan 0.70027302        nan 0.70024444\n",
      "        nan 0.70384444        nan 0.70304762        nan 0.70224127\n",
      "        nan 0.70541587        nan 0.70383492        nan 0.70701587\n",
      "        nan 0.71060635        nan 0.7066127         nan 0.70144444\n",
      "        nan 0.70500952        nan 0.7034254         nan 0.71058413\n",
      "        nan 0.70701587        nan 0.7058127         nan 0.71098095\n",
      "        nan 0.70781587        nan 0.7038254         nan 0.70501905\n",
      "        nan 0.70581587        nan 0.70423492        nan 0.70780635\n",
      "        nan 0.70661905        nan 0.69586984        nan 0.7022254\n",
      "        nan 0.70302857        nan 0.7062            nan 0.69785714\n",
      "        nan 0.70582222        nan 0.70464127        nan 0.7058\n",
      "        nan 0.69781905        nan 0.70024762        nan 0.70543492\n",
      "        nan 0.70657778        nan 0.69902857        nan 0.69982222\n",
      "        nan 0.6994254         nan 0.70379365        nan 0.70141905\n",
      "        nan 0.69903175        nan 0.70299048        nan 0.70899048\n",
      "        nan 0.70935556        nan 0.69466349        nan 0.6982254\n",
      "        nan 0.70939048        nan 0.69506349        nan 0.69940952\n",
      "        nan 0.69783492        nan 0.70616825        nan 0.70220317\n",
      "        nan 0.70379048        nan 0.7050254         nan 0.70500317\n",
      "        nan 0.69585397        nan 0.70898413        nan 0.70815238\n",
      "        nan 0.70738413        nan 0.69902857        nan 0.69666032\n",
      "        nan 0.70859048        nan 0.70579683        nan 0.7010381\n",
      "        nan 0.70583175        nan 0.70221905        nan 0.7085873\n",
      "        nan 0.69502857        nan 0.69902857        nan 0.69783492\n",
      "        nan 0.69585397        nan 0.69944127        nan 0.69744444\n",
      "        nan 0.70421587        nan 0.7026127         nan 0.6998254\n",
      "        nan 0.70061587        nan 0.7038            nan 0.70061905\n",
      "        nan 0.69821905        nan 0.7065873         nan 0.70457778\n",
      "        nan 0.70579683        nan 0.69786667        nan 0.7029873\n",
      "        nan 0.7034254         nan 0.7038            nan 0.70062222\n",
      "        nan 0.70100635        nan 0.69545397        nan 0.7038\n",
      "        nan 0.69664127        nan 0.70979048        nan 0.70500317\n",
      "        nan 0.70579365        nan 0.7042127         nan 0.70580952\n",
      "        nan 0.7018127         nan 0.70779048        nan 0.6950381\n",
      "        nan 0.70497778        nan 0.70699048        nan 0.70542222]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       309\n",
      "           1       0.67      0.67      0.67       319\n",
      "\n",
      "    accuracy                           0.66       628\n",
      "   macro avg       0.66      0.66      0.66       628\n",
      "weighted avg       0.66      0.66      0.66       628\n",
      "\n",
      "RandomForestClassifier(max_depth=5, min_samples_leaf=2, min_samples_split=10,\n",
      "                       n_estimators=200, oob_score=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.67671289 0.68062219 0.68231259 0.67534858 0.69621814 0.6883958\n",
      " 0.69578711 0.68232384 0.67884558 0.67887556 0.68058471 0.68275112\n",
      " 0.68493253 0.68363568 0.68276987 0.68362819 0.68494378 0.68407421\n",
      " 0.68754873 0.68058096 0.68360945 0.68623313 0.68361694 0.68882309\n",
      " 0.69014618 0.68711019 0.68405922 0.69188156 0.69186657 0.68148426\n",
      " 0.68407421 0.68926162 0.68012744 0.68754123 0.67971889 0.67279235\n",
      " 0.67842579 0.6832084  0.68797976 0.684494   0.68102324 0.68275487\n",
      " 0.68534858 0.68577586 0.69403298 0.67885682 0.68189655 0.68405922\n",
      " 0.68446777 0.68667916 0.68579085 0.68754123 0.68579835 0.6896964\n",
      " 0.6897039  0.69143928 0.68884933 0.6871027  0.69100075 0.68755247\n",
      " 0.68016867 0.68753373 0.68580585 0.68796477 0.67972639 0.6823051\n",
      " 0.68795352 0.68405547 0.68709895 0.68970765 0.68622564 0.68665292\n",
      " 0.6823051  0.67537481 0.68362819 0.67625187 0.68492879 0.68018741\n",
      " 0.68535607 0.67884558 0.67712894 0.68362444 0.67756372 0.67972639\n",
      " 0.68578336 0.68493628 0.69187781 0.68578336 0.68491004 0.67881934\n",
      " 0.68797601 0.68751124 0.68450525 0.69014243 0.67754873 0.68057721\n",
      " 0.67627436 0.68318216 0.68016492 0.68751499 0.69318966 0.67883433\n",
      " 0.687991   0.68968891 0.69145052 0.67973388 0.67322339 0.69016117\n",
      " 0.68146177 0.68670165 0.68144303 0.68577961 0.6745015  0.6858096\n",
      " 0.68402549 0.69186282 0.68752624 0.6871027  0.69056222 0.68405547\n",
      " 0.68666417 0.68276987 0.68362069 0.6805922  0.68754123 0.68795352\n",
      " 0.69359445 0.69101199 0.67927661 0.68275862 0.68667166 0.68408171\n",
      " 0.68231259 0.68797601 0.69144303 0.68230885 0.67975262 0.6831934\n",
      " 0.68839955 0.68665667 0.68756372 0.67884933 0.69273238 0.68796102\n",
      " 0.68230135 0.68363568 0.68926162 0.69013118 0.6936057  0.68881559\n",
      " 0.69011619 0.68928036 0.68709145 0.68405547 0.68231634 0.68623313\n",
      " 0.69361694 0.68796102 0.69446402 0.69056222 0.68925412 0.690997\n",
      " 0.68712519 0.68707646 0.68925787 0.68839205 0.69404048 0.68796102\n",
      " 0.68274363 0.68795352 0.68188156 0.68621814 0.68577586 0.69011994\n",
      " 0.68881184 0.68970015 0.6823051  0.68751874 0.68842204 0.69446027\n",
      " 0.68967766 0.69360945 0.68837706 0.69187031 0.68187031 0.6948913\n",
      " 0.6862069  0.68535607 0.69577586 0.68926162 0.68621814 0.69013493\n",
      " 0.68925037 0.68883808 0.69272864 0.68187781 0.68883058 0.6883958\n",
      " 0.69056972 0.68926162 0.68798726 0.69402924 0.68708021 0.68536732\n",
      " 0.68797601 0.68926912 0.69101199 0.68664918 0.69141679 0.68275862\n",
      " 0.69187781 0.68711769 0.68754123 0.68534108 0.69055847 0.6870952\n",
      " 0.68971514 0.68665667 0.68317091 0.68838831 0.69188156 0.68968516\n",
      " 0.69055472 0.68925412 0.6857946  0.67973013 0.68622564 0.68579085\n",
      " 0.68968516 0.68708771 0.68882684 0.69098576 0.69098951 0.68534858\n",
      " 0.68274363 0.68623313 0.69577586 0.68493253 0.69142804 0.68752999\n",
      " 0.68361319 0.6910045  0.68711769 0.69621439 0.69273238 0.69012744\n",
      " 0.6870952  0.6949063  0.68754123 0.68231259 0.68276612 0.67969265\n",
      " 0.68623313 0.68534858 0.6896964  0.68706897 0.6914018  0.68793853\n",
      " 0.68622939 0.68667166 0.68448651 0.68752249 0.67971514 0.68926912\n",
      " 0.68754498 0.68404423 0.68968516 0.69058096 0.68581709 0.6871027\n",
      " 0.68967766 0.68968516 0.69101199 0.68664543 0.68970015 0.69357946\n",
      " 0.68534483 0.69361319 0.68883058 0.68883058 0.68883808 0.68621064\n",
      " 0.68232384 0.68969265 0.69186282 0.68796102 0.68579085 0.68448276\n",
      " 0.69143553 0.67754498 0.68838831 0.68705772 0.68970765 0.68449775\n",
      " 0.6935907  0.68058846 0.67665667 0.68580585 0.6871027  0.68796477\n",
      " 0.68622564 0.68883058 0.69145427 0.6832009  0.69446402 0.68406297\n",
      " 0.68406297 0.68621064 0.68709145 0.68363568 0.68665292 0.67799475\n",
      " 0.68796102 0.68841454 0.69011619 0.67754123 0.69231634 0.68492504\n",
      " 0.68621064 0.6861994  0.67623313 0.68795727 0.68534108 0.68059595\n",
      " 0.69185532 0.68706147 0.68794978 0.69233508 0.68447151 0.68103448\n",
      " 0.67797976 0.68841079 0.6901012  0.68534108 0.68926162 0.68230135\n",
      " 0.68404423 0.69275487 0.6914093  0.68665292 0.69013868 0.68492879\n",
      " 0.68448276 0.69012744 0.68016867 0.68015367 0.68534858 0.68536357\n",
      " 0.67883808 0.69229385 0.68623313 0.68318216 0.67233133 0.67973013\n",
      " 0.6957946  0.68321589 0.68106072 0.69667166 0.6935982  0.6936057\n",
      " 0.67625562 0.68619565 0.68014993 0.68275862 0.68403298 0.68537106\n",
      " 0.69230135 0.68233508 0.68968891 0.68625187 0.68277361 0.68447901\n",
      " 0.68840705 0.68277361 0.687991   0.68754873 0.68971514 0.68798351\n",
      " 0.6849063  0.69185157 0.6858021  0.68577211 0.67841829 0.68971514\n",
      " 0.68623313 0.68189655 0.68668291 0.68232009 0.68362069 0.68793103\n",
      " 0.69056222 0.68535607 0.68666417 0.69318216 0.68363568 0.68057721\n",
      " 0.68882684 0.6857946  0.69403298 0.68363568 0.68666417 0.69102324\n",
      " 0.68754123 0.69015367 0.68797601 0.6871027  0.68144303 0.68666417\n",
      " 0.6922976  0.69793478 0.68841079 0.69143928 0.69186657 0.68795727\n",
      " 0.68796102 0.68361319 0.6831934  0.68144303 0.67885307 0.68449775\n",
      " 0.68926912 0.69404423 0.68796102 0.69186657 0.67928411 0.68278861\n",
      " 0.69101949 0.68839955 0.68878936 0.68449025 0.69100825 0.6935982\n",
      "        nan 0.67581709        nan 0.68186657        nan 0.68451274\n",
      "        nan 0.6831934         nan 0.67665667        nan 0.68407796\n",
      "        nan 0.68276612        nan 0.68709895        nan 0.68187781\n",
      "        nan 0.68709895        nan 0.68491379        nan 0.68187781\n",
      "        nan 0.67276987        nan 0.67319715        nan 0.68969265\n",
      "        nan 0.68015367        nan 0.68146177        nan 0.68232384\n",
      "        nan 0.69663793        nan 0.6849063         nan 0.6806072\n",
      "        nan 0.6831934         nan 0.68797226        nan 0.68102699\n",
      "        nan 0.68667541        nan 0.68405547        nan 0.68883058\n",
      "        nan 0.68668666        nan 0.67841454        nan 0.68493253\n",
      "        nan 0.68058471        nan 0.68622939        nan 0.68186282\n",
      "        nan 0.68925787        nan 0.68493628        nan 0.68406672\n",
      "        nan 0.67410045        nan 0.67712144        nan 0.68101574\n",
      "        nan 0.6836057         nan 0.67884183        nan 0.68408171\n",
      "        nan 0.69230885        nan 0.67798351        nan 0.6758096\n",
      "        nan 0.68103448        nan 0.68015742        nan 0.68885307\n",
      "        nan 0.67883808        nan 0.68664918        nan 0.6858021\n",
      "        nan 0.68319715        nan 0.68143178        nan 0.67843328\n",
      "        nan 0.67928411        nan 0.68405922        nan 0.67885307\n",
      "        nan 0.68622939        nan 0.68057721        nan 0.68014618\n",
      "        nan 0.69101199        nan 0.68361694        nan 0.68926162\n",
      "        nan 0.68623688        nan 0.68667916        nan 0.68494378\n",
      "        nan 0.6870952         nan 0.68666417        nan 0.67884558\n",
      "        nan 0.68363568        nan 0.68186657        nan 0.68666417\n",
      "        nan 0.68839205        nan 0.69404423        nan 0.69099325\n",
      "        nan 0.69533358        nan 0.68708021        nan 0.6961919\n",
      "        nan 0.69100825        nan 0.69098201        nan 0.68230135\n",
      "        nan 0.68362069        nan 0.69098576        nan 0.68796102\n",
      "        nan 0.68795727        nan 0.68924663        nan 0.69143178\n",
      "        nan 0.68795352        nan 0.68838456        nan 0.69316342\n",
      "        nan 0.69098576        nan 0.68838456        nan 0.68448651\n",
      "        nan 0.69228261        nan 0.68927661        nan 0.6884033\n",
      "        nan 0.69014618        nan 0.68537106        nan 0.68624063\n",
      "        nan 0.69142054        nan 0.6831934         nan 0.68839205\n",
      "        nan 0.69273238        nan 0.69099325        nan 0.69577586\n",
      "        nan 0.68491379        nan 0.6948913         nan 0.69315217\n",
      "        nan 0.6884033         nan 0.69143553        nan 0.69620315\n",
      "        nan 0.69402549        nan 0.68534858        nan 0.69100825\n",
      "        nan 0.69274738        nan 0.69315967        nan 0.68534483\n",
      "        nan 0.69664168        nan 0.68968516        nan 0.68968141\n",
      "        nan 0.68362444        nan 0.69577961        nan 0.69404423\n",
      "        nan 0.68925037        nan 0.6896964         nan 0.68882684\n",
      "        nan 0.69100825        nan 0.69271364        nan 0.69358696\n",
      "        nan 0.68752999        nan 0.69317466        nan 0.69097826\n",
      "        nan 0.68927661        nan 0.68665667        nan 0.69143553\n",
      "        nan 0.6949063         nan 0.69228636        nan 0.69532609\n",
      "        nan 0.68491004        nan 0.68795352        nan 0.68489505\n",
      "        nan 0.69230885        nan 0.68925037        nan 0.68793478\n",
      "        nan 0.67492504        nan 0.68186282        nan 0.68970015\n",
      "        nan 0.6857946         nan 0.66974138        nan 0.68231259\n",
      "        nan 0.68187406        nan 0.68621439        nan 0.69143178\n",
      "        nan 0.68709895        nan 0.6836057         nan 0.6870952\n",
      "        nan 0.67884183        nan 0.68230135        nan 0.68187781\n",
      "        nan 0.68578336        nan 0.68578711        nan 0.68361694\n",
      "        nan 0.69058471        nan 0.68752249        nan 0.68838081\n",
      "        nan 0.68016492        nan 0.68838456        nan 0.68492504\n",
      "        nan 0.68448651        nan 0.69056222        nan 0.68710645\n",
      "        nan 0.68708396        nan 0.68537106        nan 0.69360945\n",
      "        nan 0.68623313        nan 0.68620315        nan 0.68841829\n",
      "        nan 0.67886057        nan 0.68407421        nan 0.68449025\n",
      "        nan 0.68533733        nan 0.68406297        nan 0.68839205\n",
      "        nan 0.68318216        nan 0.68666792        nan 0.67322339\n",
      "        nan 0.68621064        nan 0.69055097        nan 0.69186657\n",
      "        nan 0.68491754        nan 0.68189655        nan 0.68317841\n",
      "        nan 0.68408546        nan 0.69229385        nan 0.68970765\n",
      "        nan 0.69056597        nan 0.6832084         nan 0.68581709\n",
      "        nan 0.68231634        nan 0.6858096         nan 0.68145052\n",
      "        nan 0.68013118        nan 0.69014243        nan 0.68926162\n",
      "        nan 0.68621439        nan 0.68711769        nan 0.6896964\n",
      "        nan 0.68578336        nan 0.69013118        nan 0.68100825\n",
      "        nan 0.68449775        nan 0.67971514        nan 0.6805922\n",
      "        nan 0.69446402        nan 0.6857946         nan 0.68406672]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72       280\n",
      "           1       0.74      0.68      0.71       297\n",
      "\n",
      "    accuracy                           0.71       577\n",
      "   macro avg       0.71      0.71      0.71       577\n",
      "weighted avg       0.71      0.71      0.71       577\n",
      "\n",
      "RandomForestClassifier(max_depth=10, max_features='log2', min_samples_leaf=4,\n",
      "                       n_estimators=64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.68804493 0.69437464 0.68998192 0.69971445 0.69727775 0.69971445\n",
      " 0.68610318 0.69388445 0.68612222 0.69678755 0.69632591 0.68902056\n",
      " 0.70505425 0.69584047 0.70166571 0.6973063  0.69241862 0.69874833\n",
      " 0.69434609 0.70117552 0.69872454 0.7006996  0.69338473 0.6977727\n",
      " 0.69241862 0.69535979 0.69389872 0.69242338 0.70165144 0.69437464\n",
      " 0.6977727  0.7074624  0.69195222 0.69048163 0.70118028 0.69970493\n",
      " 0.69292785 0.70408338 0.6982629  0.70311727 0.69970969 0.6943794\n",
      " 0.69875309 0.69388445 0.69534552 0.70747192 0.69873406 0.69874833\n",
      " 0.69389872 0.695336   0.69920998 0.6987293  0.70117552 0.70359794\n",
      " 0.7040929  0.69582619 0.69680659 0.69778698 0.70165144 0.70163716\n",
      " 0.69971445 0.70117552 0.70505425 0.70117076 0.70118028 0.69341805\n",
      " 0.70749572 0.70312203 0.70214163 0.70165144 0.69875785 0.69971921\n",
      " 0.69147154 0.68758329 0.69095755 0.69436037 0.70456406 0.69340853\n",
      " 0.69827717 0.69921473 0.68806872 0.69339425 0.68999619 0.70019513\n",
      " 0.69097658 0.69630211 0.6928993  0.69924805 0.69923853 0.69973824\n",
      " 0.69535979 0.69681135 0.6982629  0.70166571 0.69971921 0.69922901\n",
      " 0.69969065 0.69291833 0.69583095 0.69776794 0.69778698 0.6948696\n",
      " 0.69048639 0.69680183 0.70408814 0.6899724  0.70118028 0.69774415\n",
      " 0.69971445 0.68953931 0.70019037 0.69874833 0.6948696  0.69342281\n",
      " 0.695336   0.69873882 0.70019989 0.69873406 0.70118028 0.69728726\n",
      " 0.69438416 0.69972397 0.69387969 0.70020464 0.69727299 0.69583095\n",
      " 0.70263183 0.69582144 0.7050495  0.68903484 0.70019989 0.70263659\n",
      " 0.70018085 0.69825814 0.70312203 0.69825814 0.70116124 0.7006758\n",
      " 0.69584047 0.7011898  0.70361698 0.69679231 0.69242338 0.70019513\n",
      " 0.69483628 0.69680183 0.70605368 0.69727775 0.70309823 0.70216067\n",
      " 0.69582144 0.70358843 0.70069484 0.69728251 0.70213687 0.70406434\n",
      " 0.70312678 0.70117076 0.69388445 0.70215115 0.69341805 0.69679231\n",
      " 0.69728251 0.70068532 0.70893299 0.69583095 0.70118504 0.69826766\n",
      " 0.6982629  0.69437464 0.69728726 0.70214163 0.69970493 0.69970969\n",
      " 0.69824862 0.69873406 0.70651532 0.69678755 0.70116124 0.70312203\n",
      " 0.69923853 0.70019513 0.70264135 0.70019989 0.6924329  0.69631639\n",
      " 0.701166   0.69337521 0.70456882 0.69923377 0.69192842 0.69972397\n",
      " 0.70311251 0.69826766 0.68999619 0.70068056 0.69581668 0.69678755\n",
      " 0.70262707 0.69776318 0.69536455 0.69582144 0.69972873 0.69532172\n",
      " 0.70361222 0.70650581 0.69728726 0.70311251 0.68999143 0.69195222\n",
      " 0.69485532 0.70408814 0.70164668 0.69972397 0.69630211 0.69970969\n",
      " 0.69973824 0.69242814 0.69583571 0.70312203 0.70068532 0.69923853\n",
      " 0.69972873 0.6977727  0.69095755 0.69534552 0.70602989 0.70262707\n",
      " 0.701166   0.69922425 0.70165144 0.69582144 0.69922425 0.69776794\n",
      " 0.69340377 0.69583571 0.69387969 0.69825814 0.695336   0.69776794\n",
      " 0.69923853 0.70456882 0.69728251 0.70213687 0.70261755 0.70164668\n",
      " 0.69971921 0.69679231 0.69386065 0.68708357 0.701166   0.69824386\n",
      " 0.70166096 0.69535979 0.69583095 0.69825814 0.69580716 0.70214639\n",
      " 0.6943794  0.69775366 0.70167047 0.69678755 0.6987293  0.7021226\n",
      " 0.69582144 0.69195222 0.7021226  0.69097183 0.70457834 0.69922901\n",
      " 0.69921949 0.701166   0.69387969 0.69970493 0.69971921 0.69874833\n",
      " 0.69582144 0.69682086 0.69728726 0.69681135 0.69678755 0.69873406\n",
      " 0.69242814 0.69680183 0.70408814 0.70601561 0.70164192 0.69874358\n",
      " 0.69533124 0.69097183 0.69777746 0.69680659 0.69534552 0.69389397\n",
      " 0.70069484 0.69631163 0.69437464 0.69387493 0.69485532 0.69679231\n",
      " 0.69922901 0.69436512 0.69729202 0.69824862 0.69144774 0.70068056\n",
      " 0.69535504 0.69631639 0.69680183 0.69629735 0.69874358 0.70117076\n",
      " 0.70553493 0.70407862 0.69581668 0.69678755 0.70117552 0.7016562\n",
      " 0.69873882 0.69680183 0.69096231 0.69242814 0.69779174 0.69241386\n",
      " 0.69486008 0.69436988 0.70408814 0.69778698 0.70071388 0.69584999\n",
      " 0.69922425 0.70263659 0.69729678 0.6982629  0.70214639 0.69824862\n",
      " 0.69436988 0.70553969 0.70893775 0.70505901 0.69873406 0.69923853\n",
      " 0.69583095 0.69581668 0.70020464 0.69048163 0.69436988 0.69972873\n",
      " 0.70020464 0.69679707 0.70116124 0.701166   0.70069008 0.69629735\n",
      " 0.70457834 0.69632115 0.69535504 0.70164668 0.70408338 0.69875785\n",
      " 0.69485056 0.69485056 0.6885494  0.69535504 0.70263659 0.6919427\n",
      " 0.69581668 0.69633067 0.695336   0.6890158  0.69779174 0.69776794\n",
      " 0.69486008 0.69290881 0.70214639 0.69144774 0.69050067 0.69196174\n",
      " 0.70311727 0.69825338 0.70019037 0.69824386 0.701166   0.69632591\n",
      " 0.70119931 0.70261755 0.69629735 0.68707881 0.70117076 0.69486484\n",
      " 0.69728251 0.69532648 0.68465163 0.69680659 0.69970969 0.69581668\n",
      " 0.69777746 0.69582619 0.70263183 0.69729202 0.70749096 0.68902056\n",
      " 0.7036027  0.69973349 0.7045831  0.69775842 0.69729678 0.70117552\n",
      " 0.69435561 0.70262707 0.70166571 0.69631639 0.69727775 0.69921949\n",
      " 0.70651057 0.70359318 0.6914525  0.69583095 0.69535028 0.69485532\n",
      " 0.7006996  0.70117076 0.69921949 0.70118504 0.70069484 0.69773939\n",
      " 0.70068532 0.70359794 0.69534552 0.68903008 0.70360746 0.70068532\n",
      "        nan 0.69389397        nan 0.69582144        nan 0.69388921\n",
      "        nan 0.68998667        nan 0.68753093        nan 0.68658862\n",
      "        nan 0.70019513        nan 0.69389872        nan 0.69338949\n",
      "        nan 0.69146202        nan 0.69434609        nan 0.69049115\n",
      "        nan 0.70019037        nan 0.69193318        nan 0.69631163\n",
      "        nan 0.69581668        nan 0.69195222        nan 0.68949172\n",
      "        nan 0.69923853        nan 0.69678755        nan 0.695336\n",
      "        nan 0.69338473        nan 0.69678755        nan 0.69728726\n",
      "        nan 0.69193318        nan 0.68998192        nan 0.69438416\n",
      "        nan 0.70118504        nan 0.69631163        nan 0.695336\n",
      "        nan 0.70019037        nan 0.69583095        nan 0.69532172\n",
      "        nan 0.70115648        nan 0.69680183        nan 0.69873406\n",
      "        nan 0.69726347        nan 0.69487436        nan 0.69583571\n",
      "        nan 0.69387493        nan 0.68126309        nan 0.69534552\n",
      "        nan 0.68852085        nan 0.69826766        nan 0.68999143\n",
      "        nan 0.69726347        nan 0.69875785        nan 0.6914525\n",
      "        nan 0.69338949        nan 0.69389397        nan 0.69923853\n",
      "        nan 0.69486008        nan 0.68369027        nan 0.69677803\n",
      "        nan 0.69337997        nan 0.6982629         nan 0.70068056\n",
      "        nan 0.70018561        nan 0.6894822         nan 0.70312203\n",
      "        nan 0.70165144        nan 0.6958024         nan 0.69730154\n",
      "        nan 0.701166          nan 0.69580716        nan 0.70503522\n",
      "        nan 0.70020464        nan 0.70019989        nan 0.69923377\n",
      "        nan 0.69293261        nan 0.70068532        nan 0.70263659\n",
      "        nan 0.69436037        nan 0.6948458         nan 0.69775842\n",
      "        nan 0.69873406        nan 0.69971445        nan 0.70312678\n",
      "        nan 0.69484104        nan 0.70117076        nan 0.70410242\n",
      "        nan 0.69195698        nan 0.7040929         nan 0.69629735\n",
      "        nan 0.70117076        nan 0.70164192        nan 0.70263183\n",
      "        nan 0.69486008        nan 0.69872454        nan 0.70019989\n",
      "        nan 0.69725871        nan 0.69921949        nan 0.70260803\n",
      "        nan 0.69828193        nan 0.69970493        nan 0.69871502\n",
      "        nan 0.69874833        nan 0.69874833        nan 0.70117552\n",
      "        nan 0.69679707        nan 0.6924329         nan 0.70309347\n",
      "        nan 0.70019513        nan 0.70264611        nan 0.70019037\n",
      "        nan 0.7040691         nan 0.69389397        nan 0.69775842\n",
      "        nan 0.69630211        nan 0.69776794        nan 0.70508281\n",
      "        nan 0.70068532        nan 0.69193318        nan 0.69827242\n",
      "        nan 0.69920998        nan 0.69874833        nan 0.69581192\n",
      "        nan 0.69728251        nan 0.69632115        nan 0.69971445\n",
      "        nan 0.69677327        nan 0.70312678        nan 0.70505425\n",
      "        nan 0.69193794        nan 0.69924805        nan 0.69047687\n",
      "        nan 0.70262231        nan 0.70115648        nan 0.69387493\n",
      "        nan 0.69971445        nan 0.69922901        nan 0.70358843\n",
      "        nan 0.69973349        nan 0.7036027         nan 0.69872454\n",
      "        nan 0.69922425        nan 0.69339425        nan 0.70213211\n",
      "        nan 0.69582619        nan 0.69581192        nan 0.69727775\n",
      "        nan 0.69776318        nan 0.70701028        nan 0.69775366\n",
      "        nan 0.69924329        nan 0.69001999        nan 0.70796212\n",
      "        nan 0.69240434        nan 0.68900152        nan 0.69487436\n",
      "        nan 0.69581668        nan 0.69243765        nan 0.69973349\n",
      "        nan 0.70361222        nan 0.69777746        nan 0.69921473\n",
      "        nan 0.69048639        nan 0.69244241        nan 0.69631163\n",
      "        nan 0.69776794        nan 0.70311727        nan 0.69387493\n",
      "        nan 0.69973824        nan 0.69436037        nan 0.69923377\n",
      "        nan 0.69145726        nan 0.70214639        nan 0.69827242\n",
      "        nan 0.695336          nan 0.69873882        nan 0.70457358\n",
      "        nan 0.70650105        nan 0.6977727         nan 0.69485056\n",
      "        nan 0.69776318        nan 0.69485056        nan 0.69533124\n",
      "        nan 0.69534552        nan 0.69629735        nan 0.69776794\n",
      "        nan 0.69584047        nan 0.69293261        nan 0.68707405\n",
      "        nan 0.70167047        nan 0.69825814        nan 0.69728726\n",
      "        nan 0.69874358        nan 0.70456882        nan 0.69581192\n",
      "        nan 0.69777746        nan 0.69825814        nan 0.69341805\n",
      "        nan 0.69340853        nan 0.69968589        nan 0.70164192\n",
      "        nan 0.69680183        nan 0.70117076        nan 0.69535028\n",
      "        nan 0.70018085        nan 0.69728251        nan 0.69680183\n",
      "        nan 0.69920998        nan 0.6948458         nan 0.6943794\n",
      "        nan 0.69000095        nan 0.6977727         nan 0.69680183\n",
      "        nan 0.69678755        nan 0.69337045        nan 0.70652008\n",
      "        nan 0.69921949        nan 0.69824862        nan 0.68853988\n",
      "        nan 0.69485532        nan 0.6909861         nan 0.70746716]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       257\n",
      "           1       0.68      0.67      0.67       258\n",
      "\n",
      "    accuracy                           0.68       515\n",
      "   macro avg       0.68      0.68      0.68       515\n",
      "weighted avg       0.68      0.68      0.68       515\n",
      "\n",
      "RandomForestClassifier(max_depth=10, min_samples_leaf=4, oob_score=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "4320 fits failed out of a total of 17280.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4320 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 434, in fit\n",
      "    raise ValueError(\"Out of bag estimation only available if bootstrap=True\")\n",
      "ValueError: Out of bag estimation only available if bootstrap=True\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/jeffreysachs/Documents/ncaa_predictor/lib/python3.8/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.70082354 0.70025536 0.70530516 0.69798902 0.70419433 0.69406282\n",
      " 0.7042071  0.69968718 0.69689096 0.70022344 0.69911261 0.69859551\n",
      " 0.71151685 0.70591803 0.70758427 0.71040603 0.7109359  0.70187053\n",
      " 0.70698417 0.70024898 0.70475613 0.69971272 0.69853805 0.70530516\n",
      " 0.70081716 0.70253447 0.70361338 0.69964888 0.69966164 0.70365807\n",
      " 0.69126021 0.70473698 0.68783197 0.69740169 0.70980592 0.70362615\n",
      " 0.69968718 0.71151047 0.70755235 0.70309627 0.7047689  0.69461823\n",
      " 0.70815884 0.69460546 0.70415603 0.70533708 0.70365807 0.70307074\n",
      " 0.70585419 0.69630363 0.69856997 0.70868871 0.70590526 0.70135342\n",
      " 0.70926966 0.71099336 0.69909985 0.69576098 0.7008427  0.70926966\n",
      " 0.70644791 0.70646706 0.70702247 0.70985061 0.70533069 0.70420072\n",
      " 0.69684627 0.69348825 0.7047689  0.7058478  0.71321502 0.70473059\n",
      " 0.7092952  0.69805924 0.68957482 0.69180286 0.70362615 0.69353933\n",
      " 0.70533069 0.7081333  0.70531154 0.699119   0.70530516 0.70307074\n",
      " 0.699119   0.69573544 0.7042071  0.69687181 0.70644152 0.70024898\n",
      " 0.70473698 0.70251532 0.70642237 0.70927605 0.7087334  0.70365807\n",
      " 0.70306435 0.69516088 0.70024898 0.70249617 0.7036453  0.69572906\n",
      " 0.70705439 0.71549413 0.70021706 0.69406282 0.70247063 0.69964888\n",
      " 0.69463739 0.70699055 0.70926966 0.7008427  0.69689734 0.70529239\n",
      " 0.69969356 0.70140449 0.70024259 0.70985061 0.70139811 0.7042071\n",
      " 0.70137257 0.70983784 0.69968718 0.69800817 0.70416879 0.70080439\n",
      " 0.70759704 0.70249617 0.70081716 0.69570991 0.71038687 0.70473059\n",
      " 0.70135342 0.70134065 0.70361977 0.70478166 0.6985572  0.6940309\n",
      " 0.70254086 0.70081078 0.70195991 0.70646706 0.70643514 0.70024898\n",
      " 0.70306435 0.69515449 0.7036453  0.69629724 0.70021706 0.70022344\n",
      " 0.69853166 0.69968718 0.69909985 0.69969356 0.70308989 0.70140449\n",
      " 0.70358146 0.69462462 0.7019216  0.69742084 0.70249617 0.70416879\n",
      " 0.70022344 0.70246425 0.69798902 0.69684627 0.70023621 0.70134065\n",
      " 0.69853166 0.70191522 0.69569076 0.70416879 0.70305158 0.70925689\n",
      " 0.69684627 0.70023621 0.69796987 0.69233912 0.70247063 0.69907431\n",
      " 0.70363253 0.70132789 0.70078524 0.69686542 0.70418795 0.69687819\n",
      " 0.69911261 0.70248979 0.70137257 0.69628447 0.69516088 0.69909985\n",
      " 0.70527324 0.70303243 0.70189607 0.70363892 0.70246425 0.6979571\n",
      " 0.69741445 0.70081078 0.70194076 0.70697778 0.69909985 0.70192799\n",
      " 0.70362615 0.70021706 0.70076609 0.69909985 0.70137257 0.70192799\n",
      " 0.69966164 0.69800817 0.6996808  0.69462462 0.70191522 0.70642875\n",
      " 0.70530516 0.69961057 0.69461823 0.70078524 0.69800179 0.69907431\n",
      " 0.7047689  0.69740807 0.69570352 0.70418156 0.69803371 0.70363892\n",
      " 0.69909985 0.70195352 0.70021706 0.699119   0.69742084 0.70252809\n",
      " 0.6985572  0.69688458 0.70194076 0.69915092 0.70021067 0.70193437\n",
      " 0.70022983 0.69514173 0.70082354 0.70478166 0.6996808  0.69796987\n",
      " 0.70079801 0.6973953  0.69628447 0.70360061 0.69570352 0.6968335\n",
      " 0.69460546 0.69798902 0.69683989 0.69685266 0.70474336 0.69631639\n",
      " 0.69851251 0.69571629 0.70187692 0.69574183 0.69572906 0.699119\n",
      " 0.70416241 0.68958121 0.70023621 0.69798902 0.69461823 0.7030835\n",
      " 0.69346272 0.70812692 0.70360061 0.70528601 0.69685266 0.69574183\n",
      " 0.69631639 0.6985572  0.70078524 0.69853805 0.69293284 0.70243233\n",
      " 0.69911261 0.70142365 0.7109359  0.69062819 0.69292007 0.69347549\n",
      " 0.69294561 0.70137896 0.70026813 0.70754597 0.70137257 0.71096144\n",
      " 0.70140449 0.71207227 0.6991573  0.70703524 0.6895429  0.69746553\n",
      " 0.70813968 0.7075715  0.69856359 0.7030452  0.70082354 0.69235827\n",
      " 0.69856997 0.69683989 0.70759065 0.70474974 0.70927605 0.70363253\n",
      " 0.70418156 0.70252809 0.70417518 0.69971272 0.70474974 0.70646067\n",
      " 0.70421348 0.7042071  0.70306435 0.70586057 0.70138534 0.71092952\n",
      " 0.70699055 0.70814607 0.70307074 0.70479443 0.70081716 0.70365807\n",
      " 0.69912538 0.70022983 0.70872702 0.71264683 0.70193437 0.70418795\n",
      " 0.70361977 0.69967441 0.70418795 0.69570991 0.694631   0.70137257\n",
      " 0.69857635 0.70756512 0.70136619 0.70757789 0.7086951  0.70082354\n",
      " 0.70081078 0.69856359 0.70363892 0.70537538 0.6945927  0.70078524\n",
      " 0.70588611 0.69798902 0.70983146 0.70531154 0.70081078 0.70871425\n",
      " 0.68954928 0.70190884 0.70197906 0.70305158 0.70420072 0.70080439\n",
      " 0.70082993 0.70473698 0.70080439 0.69629086 0.7019216  0.70586696\n",
      " 0.69688458 0.71095506 0.70196629 0.69800179 0.70081078 0.70307712\n",
      " 0.69969995 0.70417518 0.70137257 0.7058478  0.70305797 0.69798264\n",
      " 0.71265322 0.69796987 0.70419433 0.70754597 0.70195991 0.70255363\n",
      " 0.70588611 0.69632278 0.70587334 0.70078524 0.70871425 0.70527324\n",
      " 0.70084908 0.70247702 0.69629086 0.70307074 0.70811415 0.70533708\n",
      " 0.69796348 0.70141088 0.70193437 0.70647983 0.70195352 0.70755235\n",
      " 0.68444203 0.69911261 0.69858274 0.70646067 0.699119   0.70534346\n",
      " 0.70308989 0.70249617 0.70473059 0.70529239 0.70586057 0.70024259\n",
      " 0.69911261 0.69913815 0.70927605 0.70534346 0.69404367 0.70246425\n",
      " 0.69913177 0.69854443 0.70646067 0.70137896 0.70307712 0.7070097\n",
      "        nan 0.69627809        nan 0.69573544        nan 0.70252171\n",
      "        nan 0.70027451        nan 0.70590526        nan 0.7008427\n",
      "        nan 0.70535623        nan 0.70759704        nan 0.70422625\n",
      "        nan 0.69968718        nan 0.70420072        nan 0.70135981\n",
      "        nan 0.69461823        nan 0.7042071         nan 0.69911261\n",
      "        nan 0.70365807        nan 0.69969356        nan 0.6979571\n",
      "        nan 0.70533708        nan 0.70644791        nan 0.69293284\n",
      "        nan 0.69631639        nan 0.70084908        nan 0.70306435\n",
      "        nan 0.70586696        nan 0.70136619        nan 0.69743361\n",
      "        nan 0.70079801        nan 0.70139173        nan 0.70301966\n",
      "        nan 0.70817799        nan 0.70925051        nan 0.70248979\n",
      "        nan 0.70301966        nan 0.699119          nan 0.70421987\n",
      "        nan 0.69969995        nan 0.70645429        nan 0.69969995\n",
      "        nan 0.70197268        nan 0.69917007        nan 0.70760342\n",
      "        nan 0.70026813        nan 0.70756512        nan 0.69634193\n",
      "        nan 0.70592441        nan 0.70138534        nan 0.69745276\n",
      "        nan 0.69574821        nan 0.70085546        nan 0.69966803\n",
      "        nan 0.70647344        nan 0.69297753        nan 0.70139811\n",
      "        nan 0.70080439        nan 0.69798902        nan 0.70248979\n",
      "        nan 0.7114977         nan 0.69913815        nan 0.70310266\n",
      "        nan 0.69851251        nan 0.69401175        nan 0.69629724\n",
      "        nan 0.69911261        nan 0.70419433        nan 0.70305158\n",
      "        nan 0.70080439        nan 0.69913815        nan 0.69966803\n",
      "        nan 0.70417518        nan 0.70589888        nan 0.70139173\n",
      "        nan 0.69231997        nan 0.69516088        nan 0.69627809\n",
      "        nan 0.69629086        nan 0.69233912        nan 0.69853166\n",
      "        nan 0.70136619        nan 0.69855082        nan 0.69401813\n",
      "        nan 0.69681435        nan 0.70078524        nan 0.69797625\n",
      "        nan 0.70135342        nan 0.69800179        nan 0.69514173\n",
      "        nan 0.70416879        nan 0.69628447        nan 0.69740169\n",
      "        nan 0.69853166        nan 0.69909985        nan 0.69685904\n",
      "        nan 0.69800179        nan 0.69966803        nan 0.7019216\n",
      "        nan 0.70137257        nan 0.69852528        nan 0.69459908\n",
      "        nan 0.69966164        nan 0.69966164        nan 0.69797625\n",
      "        nan 0.69795072        nan 0.70528601        nan 0.70245787\n",
      "        nan 0.69965526        nan 0.69403728        nan 0.69907431\n",
      "        nan 0.69571629        nan 0.69742084        nan 0.6929073\n",
      "        nan 0.69738253        nan 0.69513534        nan 0.69629724\n",
      "        nan 0.69290092        nan 0.6945927         nan 0.70132789\n",
      "        nan 0.69516726        nan 0.69627809        nan 0.69742084\n",
      "        nan 0.69459908        nan 0.69855082        nan 0.69854443\n",
      "        nan 0.70195352        nan 0.70079801        nan 0.69234551\n",
      "        nan 0.69402451        nan 0.70022344        nan 0.69515449\n",
      "        nan 0.69796987        nan 0.69966803        nan 0.70137257\n",
      "        nan 0.69290092        nan 0.70586057        nan 0.69909346\n",
      "        nan 0.69064096        nan 0.70025536        nan 0.69009831\n",
      "        nan 0.69682074        nan 0.70135981        nan 0.70471144\n",
      "        nan 0.69456078        nan 0.69966164        nan 0.69853166\n",
      "        nan 0.7075715         nan 0.69969356        nan 0.70476251\n",
      "        nan 0.70418156        nan 0.70366445        nan 0.70024259\n",
      "        nan 0.7070097         nan 0.70194076        nan 0.70082354\n",
      "        nan 0.69969356        nan 0.70648621        nan 0.69911261\n",
      "        nan 0.70538815        nan 0.69969995        nan 0.70306435\n",
      "        nan 0.70588611        nan 0.70473059        nan 0.70303243\n",
      "        nan 0.70418795        nan 0.70365807        nan 0.70193437\n",
      "        nan 0.70247063        nan 0.7008427         nan 0.70192799\n",
      "        nan 0.70528601        nan 0.7030835         nan 0.70021706\n",
      "        nan 0.69856997        nan 0.70249617        nan 0.70814607\n",
      "        nan 0.70139173        nan 0.70194714        nan 0.70194076\n",
      "        nan 0.70309627        nan 0.70195991        nan 0.71038687\n",
      "        nan 0.7036453         nan 0.69971272        nan 0.70139173\n",
      "        nan 0.70081716        nan 0.69633555        nan 0.70416879\n",
      "        nan 0.70138534        nan 0.70589249        nan 0.6973953\n",
      "        nan 0.69913177        nan 0.7081333         nan 0.70139811\n",
      "        nan 0.70082354        nan 0.69239658        nan 0.70085546\n",
      "        nan 0.69577375        nan 0.69571629        nan 0.69684627\n",
      "        nan 0.70307074        nan 0.70248979        nan 0.70083631\n",
      "        nan 0.703607          nan 0.69630363        nan 0.70196629\n",
      "        nan 0.69966803        nan 0.71208504        nan 0.70478166\n",
      "        nan 0.70529877        nan 0.70196629        nan 0.69798902\n",
      "        nan 0.70478166        nan 0.70475613        nan 0.69683989\n",
      "        nan 0.70533708        nan 0.70419433        nan 0.69742084]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.68       235\n",
      "           1       0.64      0.65      0.64       209\n",
      "\n",
      "    accuracy                           0.66       444\n",
      "   macro avg       0.66      0.66      0.66       444\n",
      "weighted avg       0.66      0.66      0.66       444\n",
      "\n",
      "RandomForestClassifier(max_features='log2', min_samples_leaf=2,\n",
      "                       n_estimators=200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/3723560689.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n"
     ]
    }
   ],
   "source": [
    "# LOOP TO FIGURE OUT BEST K \n",
    "\n",
    "# Need a function for using means that gets the mean of the last k games\n",
    "games = [5, 7, 9, 11, 13, 15, 17]\n",
    "dict_of_pred_tables = {}\n",
    "k_report = {'k': [], 'recall': [], 'precision': [], 'fp_rate': []}\n",
    "\n",
    "for k in games:\n",
    "    df = pd.read_csv('data/all_team_data.csv') # Load Data \n",
    "    # Clean Data \n",
    "    from clean_data import clean_data \n",
    "    from data_types import columns\n",
    "\n",
    "    df = df[columns] # subset to selected columms\n",
    "    df = clean_data(df) # adjust feature types, match school names, create game_ids, drop NAs \n",
    "\n",
    "    list_of_games = []\n",
    "    for game, team, id in zip(df['game'], df['school'], df['game_id']):\n",
    "        if game <= k:\n",
    "            continue\n",
    "        else: \n",
    "            current_stats = df[['game', 'date', 'game_result', 'school', 'opp_team_id', 'home', 'away', 'srs']][(df['school'] == team) & (df['game'] == game)]\n",
    "            current_stats['game_id'] = id\n",
    "            as_of_last_game = df[['streak', 'wins', 'losses']][(df['school'] == team) & (df['game'] == (game - 1))]\n",
    "            as_of_last_game['game_id'] = id\n",
    "            average_of_last_k = df[['pace', 'pts', 'opp_pts', 'ortg', 'drtg', 'ftr', '3par', 'ts_perc', 'trb_perc', 'ast_perc', 'stl_perc', \n",
    "                'blk_perc', 'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'overtimes']][(df['game'] >= (game - k)) & (df['game'] < game) & (df['school'] == team)].mean().to_frame().T\n",
    "            average_of_last_k['game_id'] = id\n",
    "            merged_row = pd.merge(pd.merge(current_stats, as_of_last_game, on='game_id'), average_of_last_k, on='game_id')\n",
    "            list_of_games.append(merged_row)\n",
    "        \n",
    "    \n",
    "    df = pd.concat(list_of_games) # merge list into df\n",
    "\n",
    "\n",
    "    # Process data \n",
    "    from adv_processing import only_duplicates, convert_percentages, convert_to_matchups\n",
    "\n",
    "    df = only_duplicates(df) # keep only duplicate game_ids\n",
    "    df = convert_percentages(df) # convert 0-100 percentages to 0-1\n",
    "    df_match = convert_to_matchups(df) # convert df table to matchup table\n",
    "\n",
    "    # Train Test Split\n",
    "\n",
    "    from train_model import split_data_to_test\n",
    "\n",
    "    # Split data based on date string for training and predictions\n",
    "    df_match_train = split_data_to_test(df_match, date='2023-02-28', type='train')\n",
    "    df_match_pred = split_data_to_test(df_match, date='2023-02-28', type='pred')\n",
    "\n",
    "    # Train Model \n",
    "    # Subset training data \n",
    "    df_match_train = df_match_train.drop(['game', 'date', 'school', 'opp_team_id', 'game_id'], axis=1)  \n",
    "\n",
    "    X = df_match_train.drop('game_result', axis=1)\n",
    "    y = df_match_train['game_result']\n",
    "    y = y.astype('int')\n",
    "\n",
    "    # split, train, test \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=235)\n",
    "\n",
    "\n",
    "    # Scale Training Data\n",
    "    X_train_scaled = preprocessor.fit_transform(X_train) # Scale Data\n",
    "    \n",
    "    # Transform Test data \n",
    "    X_test_scaled = preprocessor.transform(X_test) # Scale Data\n",
    "\n",
    "\n",
    "    # Define the hyperparameters to use with GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [64, 100, 128, 200],\n",
    "        'max_depth': [None, 5, 10],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False], \n",
    "        'oob_score': [True, False]\n",
    "        }\n",
    "\n",
    "    # Define Random Forest Classifier \n",
    "    rfc = RandomForestClassifier()\n",
    "\n",
    "    # Define Grid Search CV\n",
    "    grid = GridSearchCV(rfc, param_grid, scoring='accuracy', cv=20, n_jobs=4)\n",
    "\n",
    "    # Fit Grid Search\n",
    "    grid.fit(X_train_scaled, y_train)  \n",
    "\n",
    "\n",
    "    # Evaluate performance of Random Forests Model \n",
    "    from sklearn.metrics import classification_report\n",
    "\n",
    "    predictions = grid.predict(X_test_scaled)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    print(grid.best_estimator_)\n",
    "\n",
    "    dict_of_pred_tables[k] = df_match_pred[['game_id', 'school', 'opp_team_id', 'game_result']] # dataframe of actual results\n",
    "\n",
    "    # Subset prediction data \n",
    "    X_pred = df_match_pred.drop(['game', 'date', 'school', 'opp_team_id', 'game_id', 'game_result'], axis=1)\n",
    "\n",
    "    # Scale data\n",
    "    X_pred_scaled = preprocessor.transform(X_pred) # Scale Data\n",
    "\n",
    "    dict_of_pred_tables[k]['pred_pos'] = grid.predict(X_pred_scaled) # Add prediction\n",
    "\n",
    "    dict_of_pred_tables[k]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "    dict_of_pred_tables[k]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "\n",
    "    k_report['k'].append(k)\n",
    "    k_report['recall'].append(dict_of_pred_tables[k]['tp'].sum() / dict_of_pred_tables[k]['game_result'].sum())\n",
    "    k_report['precision'].append(dict_of_pred_tables[k]['tp'].sum() / dict_of_pred_tables[k]['pred_pos'].sum())\n",
    "    k_report['accuracy'].append(dict_of_pred_tables[k]['accuracy'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "del k_report['fp_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7208480565371025"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_pred_tables[5]['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_report['accuracy'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in games: \n",
    "    k_report['accuracy'].append(dict_of_pred_tables[k]['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_report_df = pd.DataFrame(k_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.731034</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>0.720848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.598592</td>\n",
       "      <td>0.765766</td>\n",
       "      <td>0.707746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.719101</td>\n",
       "      <td>0.718310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0.765517</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>0.721831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.700704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>0.694656</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.735915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17</td>\n",
       "      <td>0.693431</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.711268</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k    recall  precision  accuracy\n",
       "0   5  0.731034   0.726027  0.720848\n",
       "1   7  0.598592   0.765766  0.707746\n",
       "2   9  0.810127   0.719101  0.718310\n",
       "3  11  0.765517   0.711538  0.721831\n",
       "4  13  0.642857   0.767442  0.700704\n",
       "5  15  0.694656   0.722222  0.735915\n",
       "6  17  0.693431   0.703704  0.711268"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization Pipeline\n",
    "# Create a pipeline for numerical features\n",
    "num_transformer = Pipeline(steps=[('scaler', StandardScaler())]) \n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "preprocessor_pca = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, ['pts', 'opp_pts', 'ortg', 'drtg','pace', 'ftr','3par', 'ts_perc', \n",
    "                                  'trb_perc', 'ast_perc', 'stl_perc', 'blk_perc', 'streak', 'wins', 'losses',\n",
    "                                  'overtimes', 'srs',\n",
    "                                'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'opp_pts_self', 'opp_pts_team',\n",
    "                                'opp_ortg', 'opp_drtg', 'opp_pace','opp_ftr', 'opp_3par', 'opp_ts_perc', \n",
    "                                'opp_trb_perc', 'opp_ast_perc', 'opp_stl_perc', 'opp_blk_perc', \n",
    "                                'opp_efg_perc', 'opp_tov_perc', 'opp_orb_perc', 'opp_ft_fga', 'opp_streak', 'opp_wins', 'opp_losses','opp_srs',])\n",
    "        ], \n",
    "        remainder='passthrough'\n",
    "    )  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.62       373\n",
      "           1       0.67      0.68      0.68       427\n",
      "\n",
      "    accuracy                           0.65       800\n",
      "   macro avg       0.65      0.65      0.65       800\n",
      "weighted avg       0.65      0.65      0.65       800\n",
      "\n",
      "LogisticRegression(C=0.1, l1_ratio=0.5, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.59      0.61       386\n",
      "           1       0.64      0.68      0.66       414\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.63      0.63       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(C=0.001, l1_ratio=0.9, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       378\n",
      "           1       0.68      0.68      0.68       422\n",
      "\n",
      "    accuracy                           0.67       800\n",
      "   macro avg       0.66      0.66      0.66       800\n",
      "weighted avg       0.67      0.67      0.67       800\n",
      "\n",
      "LogisticRegression(C=0.001, l1_ratio=0, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.68      0.67       408\n",
      "           1       0.65      0.62      0.64       392\n",
      "\n",
      "    accuracy                           0.65       800\n",
      "   macro avg       0.65      0.65      0.65       800\n",
      "weighted avg       0.65      0.65      0.65       800\n",
      "\n",
      "LogisticRegression(C=0.01, l1_ratio=0.1, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67       424\n",
      "           1       0.63      0.64      0.64       376\n",
      "\n",
      "    accuracy                           0.66       800\n",
      "   macro avg       0.66      0.66      0.66       800\n",
      "weighted avg       0.66      0.66      0.66       800\n",
      "\n",
      "LogisticRegression(C=0.001, l1_ratio=0, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.64      0.64       402\n",
      "           1       0.64      0.64      0.64       398\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(C=0.01, l1_ratio=0.3, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.65      0.64       393\n",
      "           1       0.65      0.63      0.64       407\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(C=0.01, l1_ratio=0.4, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.63      0.64       407\n",
      "           1       0.63      0.65      0.64       393\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(C=0.01, l1_ratio=0.1, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       436\n",
      "           1       0.59      0.65      0.62       364\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(C=0.1, l1_ratio=0, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       410\n",
      "           1       0.63      0.65      0.64       390\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(l1_ratio=0, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       409\n",
      "           1       0.63      0.65      0.64       391\n",
      "\n",
      "    accuracy                           0.65       800\n",
      "   macro avg       0.65      0.65      0.65       800\n",
      "weighted avg       0.65      0.65      0.65       800\n",
      "\n",
      "LogisticRegression(C=0.1, l1_ratio=0.2, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.64      0.63       368\n",
      "           1       0.68      0.67      0.68       432\n",
      "\n",
      "    accuracy                           0.66       800\n",
      "   macro avg       0.65      0.65      0.65       800\n",
      "weighted avg       0.66      0.66      0.66       800\n",
      "\n",
      "LogisticRegression(C=10.0, l1_ratio=0.3, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.66      0.66       403\n",
      "           1       0.65      0.64      0.64       397\n",
      "\n",
      "    accuracy                           0.65       800\n",
      "   macro avg       0.65      0.65      0.65       800\n",
      "weighted avg       0.65      0.65      0.65       800\n",
      "\n",
      "LogisticRegression(C=0.1, l1_ratio=0.5, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/Users/jeffreysachs/Library/CloudStorage/Dropbox/Mac/Documents/ncaa_predictor/clean_data.py:404: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['school'] = df['school'].map(team_labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.61      0.63       399\n",
      "           1       0.63      0.68      0.66       401\n",
      "\n",
      "    accuracy                           0.64       800\n",
      "   macro avg       0.64      0.64      0.64       800\n",
      "weighted avg       0.64      0.64      0.64       800\n",
      "\n",
      "LogisticRegression(C=0.01, l1_ratio=0.9, max_iter=100000, penalty='elasticnet',\n",
      "                   solver='saga')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:112: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:113: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/2705593131.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n"
     ]
    }
   ],
   "source": [
    "# PCA LOGISTIC REGRESSION WITH K = 5 \n",
    "\n",
    "\n",
    "# Need a function for using means that gets the mean of the last k games\n",
    "k = 5\n",
    "components = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "dict_of_log_pred_tables = {}\n",
    "component_report = {'components': [], 'recall': [], 'precision': [], 'accuracy': []}\n",
    "\n",
    "for c in components:\n",
    "    df = pd.read_csv('data/all_team_data.csv') # Load Data \n",
    "    # Clean Data \n",
    "    from clean_data import clean_data \n",
    "    from data_types import columns\n",
    "\n",
    "    df = df[columns] # subset to selected columms\n",
    "    df = clean_data(df) # adjust feature types, match school names, create game_ids, drop NAs \n",
    "\n",
    "    list_of_games = []\n",
    "    for game, team, id in zip(df['game'], df['school'], df['game_id']):\n",
    "        if game <= k:\n",
    "            continue\n",
    "        else: \n",
    "            current_stats = df[['game', 'date', 'game_result', 'school', 'opp_team_id', 'home', 'away', 'srs']][(df['school'] == team) & (df['game'] == game)]\n",
    "            current_stats['game_id'] = id\n",
    "            as_of_last_game = df[['streak', 'wins', 'losses']][(df['school'] == team) & (df['game'] == (game - 1))]\n",
    "            as_of_last_game['game_id'] = id\n",
    "            average_of_last_k = df[['pace', 'pts', 'opp_pts', 'ortg', 'drtg', 'ftr', '3par', 'ts_perc', 'trb_perc', 'ast_perc', 'stl_perc', \n",
    "                'blk_perc', 'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'overtimes']][(df['game'] >= (game - 5)) & (df['game'] < game) & (df['school'] == team)].mean().to_frame().T\n",
    "            average_of_last_k['game_id'] = id\n",
    "            merged_row = pd.merge(pd.merge(current_stats, as_of_last_game, on='game_id'), average_of_last_k, on='game_id')\n",
    "            list_of_games.append(merged_row)\n",
    "        \n",
    "    \n",
    "    df = pd.concat(list_of_games) # merge list into df\n",
    "\n",
    "\n",
    "    # Process data \n",
    "    from adv_processing import only_duplicates, convert_percentages, convert_to_matchups\n",
    "\n",
    "    df = only_duplicates(df) # keep only duplicate game_ids\n",
    "    df = convert_percentages(df) # convert 0-100 percentages to 0-1\n",
    "    df_match = convert_to_matchups(df) # convert df table to matchup table\n",
    "\n",
    "    # Train Test Split\n",
    "\n",
    "    from train_model import split_data_to_test\n",
    "\n",
    "    # Split data based on date string for training and predictions\n",
    "    df_match_train = split_data_to_test(df_match, date='2023-02-28', type='train')\n",
    "    df_match_pred = split_data_to_test(df_match, date='2023-02-28', type='pred')\n",
    "\n",
    "    # Train Model \n",
    "    # Subset training data \n",
    "    df_match_train = df_match_train.drop(['game', 'date', 'school', 'opp_team_id', 'game_id'], axis=1)  \n",
    "\n",
    "    X = df_match_train.drop('game_result', axis=1)\n",
    "    y = df_match_train['game_result']\n",
    "    y = y.astype('int')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=235)\n",
    "\n",
    "    # Standardize the data \n",
    "    X_train_scaled = preprocessor_pca.fit_transform(X_train)\n",
    "    X_test_scaled = preprocessor_pca.transform(X_test)\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    #  Apply PCA\n",
    "    pca = PCA(n_components = c)\n",
    "\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "\n",
    "    # Transform the data \n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    # FOR PERFORMING LOGISTIC REGRESSION\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    # Define the model \n",
    "    model = LogisticRegression(penalty='elasticnet', solver='saga', max_iter=100000)\n",
    "\n",
    "    # Define the hyperparameters \n",
    "    params = {'C': np.logspace(-3, 3, 7),\n",
    "              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n",
    "\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    # Perform a grid search over the hyperparameters \n",
    "    grid = GridSearchCV(model, param_grid=params, cv=20, n_jobs=4, scoring='accuracy')\n",
    "    grid.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "    predictions = grid.predict(X_test_pca)\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    print(grid.best_estimator_)\n",
    "\n",
    "    dict_of_log_pred_tables[c] = df_match_pred[['game_id', 'school', 'opp_team_id', 'game_result']] # dataframe of actual results\n",
    "\n",
    "    # Subset prediction data \n",
    "    X_pred = df_match_pred.drop(['game', 'date', 'school', 'opp_team_id', 'game_id', 'game_result'], axis=1)\n",
    "\n",
    "    # Scale data\n",
    "    X_pred_scaled = preprocessor.transform(X_pred) # Scale Data\n",
    "    X_pred_pca = pca.transform(X_pred_scaled)\n",
    "\n",
    "    dict_of_log_pred_tables[c]['pred_pos'] = grid.predict(X_pred_pca) # Add prediction\n",
    "\n",
    "    prediction_probs = grid.predict_proba(X_pred_pca)\n",
    "    pos_class = []\n",
    "    for pair in prediction_probs:\n",
    "        pos_class.append(pair[1])\n",
    "\n",
    "    dict_of_log_pred_tables[c]['pred_prob'] = pos_class  # Add prediction prob\n",
    "    dict_of_log_pred_tables[c]['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "    dict_of_log_pred_tables[c]['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "\n",
    "    component_report['components'].append(c)\n",
    "    component_report['recall'].append(dict_of_log_pred_tables[c]['tp'].sum() / dict_of_log_pred_tables[c]['game_result'].sum())\n",
    "    component_report['precision'].append(dict_of_log_pred_tables[c]['tp'].sum() / dict_of_log_pred_tables[c]['pred_pos'].sum())\n",
    "    component_report['accuracy'].append(dict_of_log_pred_tables[c]['accuracy'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>components</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.590106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.460993</td>\n",
       "      <td>0.575221</td>\n",
       "      <td>0.561837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.603175</td>\n",
       "      <td>0.575972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.503311</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.598592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.467626</td>\n",
       "      <td>0.560345</td>\n",
       "      <td>0.558304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.443609</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.595070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.560976</td>\n",
       "      <td>0.556338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.489510</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.549296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.567164</td>\n",
       "      <td>0.539007</td>\n",
       "      <td>0.566901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>0.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.506667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.583039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.519380</td>\n",
       "      <td>0.475177</td>\n",
       "      <td>0.519435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.531469</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>0.531690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    components    recall  precision  accuracy\n",
       "0            2  0.422535   0.638298  0.590106\n",
       "1            3  0.460993   0.575221  0.561837\n",
       "2            4  0.520548   0.603175  0.575972\n",
       "3            5  0.503311   0.660870  0.598592\n",
       "4            6  0.467626   0.560345  0.558304\n",
       "5            7  0.443609   0.590000  0.595070\n",
       "6            8  0.489362   0.560976  0.556338\n",
       "7            9  0.489510   0.560000  0.549296\n",
       "8           10  0.567164   0.539007  0.566901\n",
       "9           11  0.585714   0.500000  0.505300\n",
       "10          12  0.500000   0.507143  0.505300\n",
       "11          13  0.506667   0.633333  0.583039\n",
       "12          14  0.519380   0.475177  0.519435\n",
       "13          15  0.531469   0.535211  0.531690"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_report_df = pd.DataFrame(component_report)\n",
    "\n",
    "c_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>school</th>\n",
       "      <th>opp_team_id</th>\n",
       "      <th>game_result</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>tp</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226646</td>\n",
       "      <td>Air Force</td>\n",
       "      <td>San Jose State</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313825</td>\n",
       "      <td>Akron</td>\n",
       "      <td>Kent State</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410568</td>\n",
       "      <td>Grambling</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>428446</td>\n",
       "      <td>Southern</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51776</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32132268</td>\n",
       "      <td>UC-Irvine</td>\n",
       "      <td>UC-Riverside</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32735846</td>\n",
       "      <td>Wofford</td>\n",
       "      <td>UNC Greensboro</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32933376</td>\n",
       "      <td>UT Arlington</td>\n",
       "      <td>Utah Valley</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33435268</td>\n",
       "      <td>Western Kentucky</td>\n",
       "      <td>UTEP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34634768</td>\n",
       "      <td>Washington State</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     game_id            school     opp_team_id game_result  pred_pos  tp  \\\n",
       "0     226646         Air Force  San Jose State           0         1   0   \n",
       "0     313825             Akron      Kent State           0         0   0   \n",
       "0     410568         Grambling     Alabama A&M           1         1   1   \n",
       "0     428446          Southern     Alabama A&M           0         1   0   \n",
       "0      51776            Auburn         Alabama           0         0   0   \n",
       "..       ...               ...             ...         ...       ...  ..   \n",
       "0   32132268         UC-Irvine    UC-Riverside           1         0   0   \n",
       "0   32735846           Wofford  UNC Greensboro           1         0   0   \n",
       "0   32933376      UT Arlington     Utah Valley           0         0   0   \n",
       "0   33435268  Western Kentucky            UTEP           1         0   0   \n",
       "0   34634768  Washington State      Washington           1         1   1   \n",
       "\n",
       "    accuracy  \n",
       "0          0  \n",
       "0          1  \n",
       "0          1  \n",
       "0          0  \n",
       "0          1  \n",
       "..       ...  \n",
       "0          0  \n",
       "0          0  \n",
       "0          1  \n",
       "0          0  \n",
       "0          1  \n",
       "\n",
       "[283 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_pred_tables[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>school</th>\n",
       "      <th>opp_team_id</th>\n",
       "      <th>game_result</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>pred_prob</th>\n",
       "      <th>tp</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226646</td>\n",
       "      <td>Air Force</td>\n",
       "      <td>San Jose State</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.873962</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31955</td>\n",
       "      <td>Akron</td>\n",
       "      <td>Ball State</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313825</td>\n",
       "      <td>Kent State</td>\n",
       "      <td>Akron</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876769</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410568</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>Grambling</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.131703</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>428446</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>Southern</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236819</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32132268</td>\n",
       "      <td>UC-Irvine</td>\n",
       "      <td>UC-Riverside</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.674466</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32735846</td>\n",
       "      <td>Wofford</td>\n",
       "      <td>UNC Greensboro</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.502435</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32933376</td>\n",
       "      <td>UT Arlington</td>\n",
       "      <td>Utah Valley</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.411495</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33435268</td>\n",
       "      <td>UTEP</td>\n",
       "      <td>Western Kentucky</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34634768</td>\n",
       "      <td>Washington State</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.549510</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     game_id            school       opp_team_id game_result  pred_pos  \\\n",
       "0     226646         Air Force    San Jose State           0         1   \n",
       "0      31955             Akron        Ball State           1         1   \n",
       "0     313825        Kent State             Akron           1         1   \n",
       "0     410568       Alabama A&M         Grambling           0         0   \n",
       "0     428446       Alabama A&M          Southern           1         0   \n",
       "..       ...               ...               ...         ...       ...   \n",
       "0   32132268         UC-Irvine      UC-Riverside           1         1   \n",
       "0   32735846           Wofford    UNC Greensboro           1         1   \n",
       "0   32933376      UT Arlington       Utah Valley           0         0   \n",
       "0   33435268              UTEP  Western Kentucky           0         0   \n",
       "0   34634768  Washington State        Washington           1         1   \n",
       "\n",
       "    pred_prob  tp  accuracy  \n",
       "0    0.873962   0         0  \n",
       "0    0.632055   1         1  \n",
       "0    0.876769   1         1  \n",
       "0    0.131703   0         1  \n",
       "0    0.236819   0         0  \n",
       "..        ...  ..       ...  \n",
       "0    0.674466   1         1  \n",
       "0    0.502435   1         1  \n",
       "0    0.411495   0         1  \n",
       "0    0.240118   0         1  \n",
       "0    0.549510   1         1  \n",
       "\n",
       "[284 rows x 8 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_of_log_pred_tables[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models_df = pd.merge(dict_of_pred_tables[5], dict_of_log_pred_tables[5].drop(['school', 'opp_team_id', 'game_result', 'tp', 'accuracy', 'pred_pos'], axis=1), on='game_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>game_id</th>\n",
       "      <th>school</th>\n",
       "      <th>opp_team_id</th>\n",
       "      <th>game_result</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>tp</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>pred_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>226646</td>\n",
       "      <td>Air Force</td>\n",
       "      <td>San Jose State</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.873962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>313825</td>\n",
       "      <td>Akron</td>\n",
       "      <td>Kent State</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.876769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>410568</td>\n",
       "      <td>Grambling</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>428446</td>\n",
       "      <td>Southern</td>\n",
       "      <td>Alabama A&amp;M</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.236819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51776</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>32132268</td>\n",
       "      <td>UC-Irvine</td>\n",
       "      <td>UC-Riverside</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.674466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>32735846</td>\n",
       "      <td>Wofford</td>\n",
       "      <td>UNC Greensboro</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>32933376</td>\n",
       "      <td>UT Arlington</td>\n",
       "      <td>Utah Valley</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.411495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>33435268</td>\n",
       "      <td>Western Kentucky</td>\n",
       "      <td>UTEP</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.240118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>34634768</td>\n",
       "      <td>Washington State</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.549510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      game_id            school     opp_team_id game_result  pred_pos  tp  \\\n",
       "0      226646         Air Force  San Jose State           0         1   0   \n",
       "1      313825             Akron      Kent State           0         0   0   \n",
       "2      410568         Grambling     Alabama A&M           1         1   1   \n",
       "3      428446          Southern     Alabama A&M           0         1   0   \n",
       "4       51776            Auburn         Alabama           0         0   0   \n",
       "..        ...               ...             ...         ...       ...  ..   \n",
       "278  32132268         UC-Irvine    UC-Riverside           1         0   0   \n",
       "279  32735846           Wofford  UNC Greensboro           1         0   0   \n",
       "280  32933376      UT Arlington     Utah Valley           0         0   0   \n",
       "281  33435268  Western Kentucky            UTEP           1         0   0   \n",
       "282  34634768  Washington State      Washington           1         1   1   \n",
       "\n",
       "     accuracy  pred_prob  \n",
       "0           0   0.873962  \n",
       "1           1   0.876769  \n",
       "2           1   0.131703  \n",
       "3           0   0.236819  \n",
       "4           1   0.831950  \n",
       "..        ...        ...  \n",
       "278         0   0.674466  \n",
       "279         0   0.502435  \n",
       "280         1   0.411495  \n",
       "281         0   0.240118  \n",
       "282         1   0.549510  \n",
       "\n",
       "[283 rows x 8 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5860"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models_df['return'] = best_models_df['accuracy'].apply(lambda x: 10 if x == 1 else -100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6 ,\n",
       "       0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7 , 0.71,\n",
       "       0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8 , 0.81, 0.82,\n",
       "       0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9 , 0.91, 0.92, 0.93,\n",
       "       0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.  ])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(0.5, 1.0, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/110973089.py:6: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  fp_rate = 1 - (subset_df['tp'].sum() / subset_df['pred_pos'].sum())\n",
      "/var/folders/3_/0tw_81z57sj1vr5y2x7dnmnm0000gn/T/ipykernel_9657/110973089.py:10: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  calibration_threshold['precision'].append((subset_df['tp'].sum() / subset_df['pred_pos'].sum()))\n"
     ]
    }
   ],
   "source": [
    "calibration_threshold = {'thresh': [], 'accuracy': [], 'fp_rate': [], 'precision': [], 'games': [], 'return': []}\n",
    "for thresh in np.linspace(0.5, 1.0, 51): \n",
    "    best_models_df['pos_neg_thresh'] = best_models_df['pred_prob'].apply(lambda x: 1 if (x > thresh) | (x <= 1-thresh) else 0)\n",
    "    best_models_df['return'] = best_models_df['accuracy'].apply(lambda x: 50 if x == 1 else -100) # Figure out how to attach actual returns\n",
    "    subset_df = best_models_df[best_models_df['pos_neg_thresh'] == 1]\n",
    "    fp_rate = 1 - (subset_df['tp'].sum() / subset_df['pred_pos'].sum())\n",
    "    calibration_threshold['thresh'].append(thresh)\n",
    "    calibration_threshold['accuracy'].append(subset_df['accuracy'].mean())\n",
    "    calibration_threshold['fp_rate'].append(fp_rate)\n",
    "    calibration_threshold['precision'].append((subset_df['tp'].sum() / subset_df['pred_pos'].sum()))\n",
    "    calibration_threshold['games'].append(subset_df['pos_neg_thresh'].sum())\n",
    "    calibration_threshold['return'].append(subset_df['return'].sum())\n",
    "\n",
    "\n",
    "thresh_cal_df = pd.DataFrame(calibration_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_cal_df['ROI'] = thresh_cal_df['return'] / (thresh_cal_df['games']*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thresh</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>games</th>\n",
       "      <th>return</th>\n",
       "      <th>ROI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.720848</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>283</td>\n",
       "      <td>2300</td>\n",
       "      <td>0.081272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>280</td>\n",
       "      <td>2450</td>\n",
       "      <td>0.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.723636</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>275</td>\n",
       "      <td>2350</td>\n",
       "      <td>0.085455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.726937</td>\n",
       "      <td>0.269504</td>\n",
       "      <td>0.730496</td>\n",
       "      <td>271</td>\n",
       "      <td>2450</td>\n",
       "      <td>0.090406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.727612</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>268</td>\n",
       "      <td>2450</td>\n",
       "      <td>0.091418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.730038</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>263</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.095057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>260</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.096154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.736220</td>\n",
       "      <td>0.267176</td>\n",
       "      <td>0.732824</td>\n",
       "      <td>254</td>\n",
       "      <td>2650</td>\n",
       "      <td>0.104331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>0.271318</td>\n",
       "      <td>0.728682</td>\n",
       "      <td>250</td>\n",
       "      <td>2450</td>\n",
       "      <td>0.098000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.724280</td>\n",
       "      <td>0.275591</td>\n",
       "      <td>0.724409</td>\n",
       "      <td>243</td>\n",
       "      <td>2100</td>\n",
       "      <td>0.086420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.720833</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>240</td>\n",
       "      <td>1950</td>\n",
       "      <td>0.081250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.61</td>\n",
       "      <td>0.722689</td>\n",
       "      <td>0.274194</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>238</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.084034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.62</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.731092</td>\n",
       "      <td>231</td>\n",
       "      <td>2100</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.721239</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.721739</td>\n",
       "      <td>226</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.081858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.717489</td>\n",
       "      <td>0.278261</td>\n",
       "      <td>0.721739</td>\n",
       "      <td>223</td>\n",
       "      <td>1700</td>\n",
       "      <td>0.076233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.714953</td>\n",
       "      <td>0.276786</td>\n",
       "      <td>0.723214</td>\n",
       "      <td>214</td>\n",
       "      <td>1550</td>\n",
       "      <td>0.072430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.715640</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>211</td>\n",
       "      <td>1550</td>\n",
       "      <td>0.073460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.707317</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.719626</td>\n",
       "      <td>205</td>\n",
       "      <td>1250</td>\n",
       "      <td>0.060976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.716418</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>201</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.074627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>196</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>192</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.71</td>\n",
       "      <td>0.724324</td>\n",
       "      <td>0.268041</td>\n",
       "      <td>0.731959</td>\n",
       "      <td>185</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.086486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.72</td>\n",
       "      <td>0.718232</td>\n",
       "      <td>0.273684</td>\n",
       "      <td>0.726316</td>\n",
       "      <td>181</td>\n",
       "      <td>1400</td>\n",
       "      <td>0.077348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>0.734043</td>\n",
       "      <td>174</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.74</td>\n",
       "      <td>0.723529</td>\n",
       "      <td>0.271739</td>\n",
       "      <td>0.728261</td>\n",
       "      <td>170</td>\n",
       "      <td>1450</td>\n",
       "      <td>0.085294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.736196</td>\n",
       "      <td>0.261364</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>163</td>\n",
       "      <td>1700</td>\n",
       "      <td>0.104294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.732484</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.732558</td>\n",
       "      <td>157</td>\n",
       "      <td>1550</td>\n",
       "      <td>0.098726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.265060</td>\n",
       "      <td>0.734940</td>\n",
       "      <td>152</td>\n",
       "      <td>1600</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.732877</td>\n",
       "      <td>0.271605</td>\n",
       "      <td>0.728395</td>\n",
       "      <td>146</td>\n",
       "      <td>1450</td>\n",
       "      <td>0.099315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.731884</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>138</td>\n",
       "      <td>1350</td>\n",
       "      <td>0.097826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.721805</td>\n",
       "      <td>0.273973</td>\n",
       "      <td>0.726027</td>\n",
       "      <td>133</td>\n",
       "      <td>1100</td>\n",
       "      <td>0.082707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.720930</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>129</td>\n",
       "      <td>1050</td>\n",
       "      <td>0.081395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.731092</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>119</td>\n",
       "      <td>1150</td>\n",
       "      <td>0.096639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>116</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.84</td>\n",
       "      <td>0.712963</td>\n",
       "      <td>0.290323</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>108</td>\n",
       "      <td>750</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.720430</td>\n",
       "      <td>0.290909</td>\n",
       "      <td>0.709091</td>\n",
       "      <td>93</td>\n",
       "      <td>750</td>\n",
       "      <td>0.080645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.86</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.288462</td>\n",
       "      <td>0.711538</td>\n",
       "      <td>88</td>\n",
       "      <td>800</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.87</td>\n",
       "      <td>0.753247</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>77</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.129870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.788732</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>71</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.183099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.89</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>60</td>\n",
       "      <td>1350</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.194444</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>59</td>\n",
       "      <td>1300</td>\n",
       "      <td>0.220339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.91</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>54</td>\n",
       "      <td>1050</td>\n",
       "      <td>0.194444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.92</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>47</td>\n",
       "      <td>850</td>\n",
       "      <td>0.180851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>40</td>\n",
       "      <td>1100</td>\n",
       "      <td>0.275000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.94</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>33</td>\n",
       "      <td>750</td>\n",
       "      <td>0.227273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>24</td>\n",
       "      <td>600</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.96</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>17</td>\n",
       "      <td>250</td>\n",
       "      <td>0.147059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>10</td>\n",
       "      <td>-100</td>\n",
       "      <td>-0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>-100</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    thresh  accuracy   fp_rate  precision  games  return       ROI\n",
       "0     0.50  0.720848  0.273973   0.726027    283    2300  0.081272\n",
       "1     0.51  0.725000  0.273973   0.726027    280    2450  0.087500\n",
       "2     0.52  0.723636  0.272727   0.727273    275    2350  0.085455\n",
       "3     0.53  0.726937  0.269504   0.730496    271    2450  0.090406\n",
       "4     0.54  0.727612  0.271429   0.728571    268    2450  0.091418\n",
       "5     0.55  0.730038  0.266667   0.733333    263    2500  0.095057\n",
       "6     0.56  0.730769  0.268657   0.731343    260    2500  0.096154\n",
       "7     0.57  0.736220  0.267176   0.732824    254    2650  0.104331\n",
       "8     0.58  0.732000  0.271318   0.728682    250    2450  0.098000\n",
       "9     0.59  0.724280  0.275591   0.724409    243    2100  0.086420\n",
       "10    0.60  0.720833  0.277778   0.722222    240    1950  0.081250\n",
       "11    0.61  0.722689  0.274194   0.725806    238    2000  0.084034\n",
       "12    0.62  0.727273  0.268908   0.731092    231    2100  0.090909\n",
       "13    0.63  0.721239  0.278261   0.721739    226    1850  0.081858\n",
       "14    0.64  0.717489  0.278261   0.721739    223    1700  0.076233\n",
       "15    0.65  0.714953  0.276786   0.723214    214    1550  0.072430\n",
       "16    0.66  0.715640  0.270270   0.729730    211    1550  0.073460\n",
       "17    0.67  0.707317  0.280374   0.719626    205    1250  0.060976\n",
       "18    0.68  0.716418  0.269231   0.730769    201    1500  0.074627\n",
       "19    0.69  0.714286  0.274510   0.725490    196    1400  0.071429\n",
       "20    0.70  0.718750  0.280000   0.720000    192    1500  0.078125\n",
       "21    0.71  0.724324  0.268041   0.731959    185    1600  0.086486\n",
       "22    0.72  0.718232  0.273684   0.726316    181    1400  0.077348\n",
       "23    0.73  0.724138  0.265957   0.734043    174    1500  0.086207\n",
       "24    0.74  0.723529  0.271739   0.728261    170    1450  0.085294\n",
       "25    0.75  0.736196  0.261364   0.738636    163    1700  0.104294\n",
       "26    0.76  0.732484  0.267442   0.732558    157    1550  0.098726\n",
       "27    0.77  0.736842  0.265060   0.734940    152    1600  0.105263\n",
       "28    0.78  0.732877  0.271605   0.728395    146    1450  0.099315\n",
       "29    0.79  0.731884  0.270270   0.729730    138    1350  0.097826\n",
       "30    0.80  0.721805  0.273973   0.726027    133    1100  0.082707\n",
       "31    0.81  0.720930  0.281690   0.718310    129    1050  0.081395\n",
       "32    0.82  0.731092  0.281250   0.718750    119    1150  0.096639\n",
       "33    0.83  0.724138  0.285714   0.714286    116    1000  0.086207\n",
       "34    0.84  0.712963  0.290323   0.709677    108     750  0.069444\n",
       "35    0.85  0.720430  0.290909   0.709091     93     750  0.080645\n",
       "36    0.86  0.727273  0.288462   0.711538     88     800  0.090909\n",
       "37    0.87  0.753247  0.276596   0.723404     77    1000  0.129870\n",
       "38    0.88  0.788732  0.209302   0.790698     71    1300  0.183099\n",
       "39    0.89  0.816667  0.194444   0.805556     60    1350  0.225000\n",
       "40    0.90  0.813559  0.194444   0.805556     59    1300  0.220339\n",
       "41    0.91  0.796296  0.205882   0.794118     54    1050  0.194444\n",
       "42    0.92  0.787234  0.193548   0.806452     47     850  0.180851\n",
       "43    0.93  0.850000  0.142857   0.857143     40    1100  0.275000\n",
       "44    0.94  0.818182  0.181818   0.818182     33     750  0.227273\n",
       "45    0.95  0.833333  0.125000   0.875000     24     600  0.250000\n",
       "46    0.96  0.764706  0.181818   0.818182     17     250  0.147059\n",
       "47    0.97  0.600000  0.333333   0.666667     10    -100 -0.100000\n",
       "48    0.98  0.000000  1.000000   0.000000      1    -100 -1.000000\n",
       "49    0.99       NaN       NaN        NaN      0       0       NaN\n",
       "50    1.00       NaN       NaN        NaN      0       0       NaN"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh_cal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split df at same date as before\n",
    "# df_train = split_data_to_test(df, date='2023-02-28', type='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalization processor\n",
    "# # Create a pipeline for numerical features\n",
    "# num_transformer2 = Pipeline(steps=[('scaler', MinMaxScaler(feature_range=(0,1)))])\n",
    "\n",
    "# # Create a column transformer to apply different transformations to different columns\n",
    "# preprocessor2 = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#             ('num', num_transformer2, ['pace', 'ftr','3par', 'ts_perc', \n",
    "#                                   'trb_perc', 'ast_perc', 'stl_perc', 'blk_perc',\n",
    "#                                 'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'srs',\n",
    "#                                 'wins', 'losses', 'streak'])\n",
    "#         ], \n",
    "#     remainder='passthrough'\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(n_neighbors=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(n_neighbors=10)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # fit a nearest neighbors model on all training games    \n",
    "# nn_df = df_train[['pace', 'ftr', '3par', 'ts_perc', 'trb_perc', 'ast_perc', 'stl_perc', 'blk_perc', \n",
    "#                  'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'srs', 'wins', 'losses', 'streak']] # Subset df to wanted features\n",
    "\n",
    "# # Use pipeline to scale numerical data Use sklearn.preprocessing MinMaxScaler\n",
    "# # scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# nn_df = preprocessor2.fit_transform(nn_df)\n",
    "\n",
    "# # Fit NearestNeighbors with k neighbors (k number of similar matchups\n",
    "\n",
    "# knn = NearestNeighbors(n_neighbors=10) # 10 most similar neighbors \n",
    "\n",
    "# knn.fit(nn_df) # Fit matchup stats  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season_stats_dict = {}\n",
    "# for name in df_train['school'].unique():\n",
    "#     dataframe = df_train[df_train['school'] == name]\n",
    "#     dataframe = dataframe[['pace', 'ftr', '3par', 'ts_perc', 'trb_perc', 'ast_perc', 'stl_perc', 'blk_perc', \n",
    "#                  'efg_perc', 'tov_perc', 'orb_perc', 'ft_fga', 'srs', 'wins', 'losses', 'streak']]\n",
    "#     dataframe = preprocessor2.transform(dataframe)\n",
    "#     season_stats_dict[name] = dataframe\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to create matchup format df\n",
    "# def sim_convert_to_matchups(df):\n",
    "#     # Turn individual team data into matchup rows based on game_id\n",
    "\n",
    "#     opponent_features = ['pace', 'ftr','3par', 'ts_perc', 'trb_perc', \n",
    "#                      'ast_perc', 'stl_perc', 'blk_perc','efg_perc', \n",
    "#                      'tov_perc', 'orb_perc', 'ft_fga', 'srs', 'wins', \n",
    "#                      'losses', 'streak', 'game_id',]\n",
    "\n",
    "#     new_opp_feat_names_dict = {'pace': 'opp_pace',\n",
    "#                            'ftr': 'opp_ftr', \n",
    "#                            '3par': 'opp_3par',\n",
    "#                            'ts_perc': 'opp_ts_perc',\n",
    "#                            'trb_perc': 'opp_trb_perc',\n",
    "#                            'ast_perc': 'opp_ast_perc',\n",
    "#                            'stl_perc': 'opp_stl_perc',\n",
    "#                            'blk_perc': 'opp_blk_perc',\n",
    "#                            'efg_perc': 'opp_efg_perc',\n",
    "#                            'tov_perc': 'opp_tov_perc',\n",
    "#                            'orb_perc': 'opp_orb_perc', \n",
    "#                            'tov_perc': 'opp_tov_perc',\n",
    "#                            'orb_perc': 'opp_orb_perc',\n",
    "#                            'ft_fga': 'opp_ft_fga',\n",
    "#                            'srs': 'opp_srs', \n",
    "#                            'wins': 'opp_wins', \n",
    "#                            'losses': 'opp_losses', \n",
    "#                            'streak': 'opp_streak',}\n",
    "    \n",
    "#     list_of_matchups = []\n",
    "#     for id, school in zip(df['game_id'], df['school']):\n",
    "#         team_row = df[(df['game_id'] == id) & (df['school'] == school)]\n",
    "#         # transform team row to normal\n",
    "#         opp_row = df[opponent_features][(df['game_id'] == id) & (df['opp_team_id'] != school)] # Gets opponent features \n",
    "#         # transform team row to normal\n",
    "#         opp_row = opp_row.rename(columns=new_opp_feat_names_dict) # new column names\n",
    "#         new_row = pd.merge(team_row, opp_row, on='game_id', how='inner') # merge team and opponent data\n",
    "#         list_of_matchups.append(new_row) # add row to matchup list\n",
    "\n",
    "#     match_up_df = pd.concat(list_of_matchups)\n",
    "\n",
    "#     return match_up_df\n",
    "\n",
    "\n",
    "# Function that bootstrap samples similar matchups n times \n",
    "# and generates a predicted probability of team_a winning\n",
    "# def simulate_and_predict(df, model, n: int):\n",
    "#     n_simulations = n\n",
    "#     simulated_outcomes = []\n",
    "\n",
    "#     for i in range(n_simulations):\n",
    "#         random_row = np.random.choice(df.shape[0])\n",
    "#         matchup = df[random_row]\n",
    "#         outcome = model.predict(matchup.reshape(1, -1))\n",
    "#         simulated_outcomes.append(outcome)\n",
    "\n",
    "#     team_win_prob = np.mean(simulated_outcomes)\n",
    "\n",
    "#     return team_win_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create table of actual outcomes to test \n",
    "# actual_outcomes = df_match_pred[['game_id', 'school', 'opp_team_id', 'game_result']]\n",
    "# win_prob = []\n",
    "# for a, b in zip(actual_outcomes['school'], actual_outcomes['opp_team_id']):\n",
    "#     # Get similar teams and games for each team \n",
    "#     distances_a, indices_a = knn.kneighbors(season_stats_dict[a]) # get distances and indices for team a\n",
    "#     distances_b, indices_b = knn.kneighbors(season_stats_dict[b]) \n",
    "\n",
    "#     similar_teams = [] # similar teams to team_a\n",
    "#     for i, indx in enumerate(indices_a): \n",
    "#         similar_teams.extend(list(indx[1:11]))\n",
    "\n",
    "#     similar_opps = []\n",
    "#     for i, indx in enumerate(indices_b): \n",
    "#         similar_opps.extend(list(indx[1:11]))\n",
    "\n",
    "#     # Game ids that involve both teams\n",
    "#     similar_game_ids = [x for x in list(df_train['game_id'].iloc[similar_teams]) if x in list(df_train['game_id'].iloc[similar_opps])]\n",
    "\n",
    "#     similar_school_df = df_train.iloc[similar_teams] # df subset by teams similar to team_a\n",
    "#     # subset of teams similar that have game_ids in similar_game_ids\n",
    "#     similar_df = similar_school_df[similar_school_df['game_id'].isin(similar_game_ids)]\n",
    "#     # drop duplicate games if they exist\n",
    "#     similar_df = similar_df.drop_duplicates()\n",
    "\n",
    "#     # Convert similar df to matchups \n",
    "\n",
    "#     # Turn individual team data into matchup rows based on game_id\n",
    "#     opponent_features = ['pace', 'ftr','3par', 'ts_perc', 'trb_perc', \n",
    "#                      'ast_perc', 'stl_perc', 'blk_perc','efg_perc', \n",
    "#                      'tov_perc', 'orb_perc', 'ft_fga', 'srs', 'wins', \n",
    "#                      'losses', 'streak','game_id']\n",
    "\n",
    "#     new_opp_feat_names_dict = {\n",
    "#                            'ftr': 'opp_ftr', \n",
    "#                            '3par': 'opp_3par',\n",
    "#                            'ts_perc': 'opp_ts_perc',\n",
    "#                            'trb_perc': 'opp_trb_perc',\n",
    "#                            'ast_perc': 'opp_ast_perc',\n",
    "#                            'stl_perc': 'opp_stl_perc',\n",
    "#                            'blk_perc': 'opp_blk_perc',\n",
    "#                            'efg_perc': 'opp_efg_perc',\n",
    "#                            'tov_perc': 'opp_tov_perc',\n",
    "#                            'orb_perc': 'opp_orb_perc', \n",
    "#                            'tov_perc': 'opp_tov_perc',\n",
    "#                            'orb_perc': 'opp_orb_perc',\n",
    "#                            'ft_fga': 'opp_ft_fga',\n",
    "#                            'srs': 'opp_srs', \n",
    "#                            'wins': 'opp_wins', \n",
    "#                            'losses': 'opp_losses', \n",
    "#                            'streak': 'opp_streak',}\n",
    "    \n",
    "#     list_of_matchups = []\n",
    "#     for id, school in zip(similar_df['game_id'], similar_df['school']):\n",
    "#         team_row = df_train[(df_train['game_id'] == id) & (df_train['school'] == school)]\n",
    "#         # transform team row to normal\n",
    "#         team_row = team_row.drop(['game', 'date', 'game_result', 'school', 'opp_team_id'], axis=1)\n",
    "\n",
    "#         opp_row = df_train[opponent_features][(df_train['game_id'] == id) & (df_train['school'] != school)] # Gets opponent features \n",
    "#         # Rename rows \n",
    "#         opp_row = opp_row.rename(columns=new_opp_feat_names_dict)\n",
    "#         # drop redundant feature\n",
    "#         opp_row = opp_row.drop('pace', axis=1) \n",
    "\n",
    "#         # merge team and opp\n",
    "#         team_opp_com = pd.merge(team_row, opp_row, on='game_id', how='inner')\n",
    "\n",
    "#         # add row to list \n",
    "#         list_of_matchups.append(team_opp_com)\n",
    "    \n",
    "#     # Concatenate list into df rows\n",
    "#     similar_matchups_df = pd.concat(list_of_matchups)\n",
    "\n",
    "#     # Scale the data \n",
    "#     similar_matchups_df = preprocessor.transform(similar_matchups_df)\n",
    "\n",
    "#     # Simulate and predict\n",
    "#     win_prob.append(simulate_and_predict(similar_matchups_df, model=grid, n=1000))    \n",
    "\n",
    "\n",
    "# # Add win probs to actual outcomes \n",
    "# actual_outcomes['pred_prob'] = win_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# win_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_outcomes['pred_pos'] = actual_outcomes['pred_prob'].apply(lambda x: 1 if x > 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_outcomes['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_outcomes['accuracy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_outcomes['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual_outcomes['tp'].sum() / actual_outcomes['pred_pos'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration_threshold = {'thresh': [], 'accuracy': [], 'fp_rate': [], 'precision': [], 'games': []}\n",
    "# for thresh in np.linspace(0, 1.0, 101): \n",
    "#     actual_outcomes['pred_pos'] = actual_outcomes['pred_prob'].apply(lambda x: 1 if x >= thresh else 0)\n",
    "#     actual_outcomes['pred_neg'] = actual_outcomes['pred_prob'].apply(lambda x: 1 if x < thresh else 0)\n",
    "#     actual_outcomes['tp'] = np.vectorize(lambda x, y: 1 if x == 1 & y == 1 else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "#     actual_outcomes['accuracy'] = np.vectorize(lambda x, y: 1 if x == y else 0)(actual_outcomes['game_result'], actual_outcomes['pred_pos'])\n",
    "#     fp_rate = 1 - (actual_outcomes['tp'].sum() / actual_outcomes['pred_pos'].sum())\n",
    "#     calibration_threshold['thresh'].append(thresh)\n",
    "#     calibration_threshold['accuracy'].append(actual_outcomes['accuracy'].mean())\n",
    "#     calibration_threshold['fp_rate'].append(fp_rate)\n",
    "#     calibration_threshold['precision'].append((actual_outcomes['tp'].sum() / actual_outcomes['pred_pos'].sum()))\n",
    "#     calibration_threshold['games'].append(actual_outcomes['pred_pos'].sum())\n",
    "\n",
    "\n",
    "# thresh_cal_df = pd.DataFrame(calibration_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh_cal_df['accuracy'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh_cal_df['precision'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh_cal_df.iloc[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncaa_predictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de3e3c86aa8aa0711b7268c931bfd5037db8de5b5aab2fec50457131e347c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
